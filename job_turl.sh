#!/bin/bash

#PBS -S /bin/bash
#PBS -N "turl_execution_test"
#PBS -q gpu
#PBS -l select=1:ncpus=48:ngpus=4,walltime=24:00:00
#PBS -k eo

##########################################################################

# Explanation of the various options:
#
# -S: select the shell to be used (BASH for us)
# -N: name of the job (used e.g. when launching qstat)
# -q: name of queue (cpu or gpu for DaVinci-1)
# -l: specifies requested resources
#     select  : number of computational nodes (max 32)
#     ncpus   : number of CPU cores per node  (max 48)
#     walltime: expected execution time (max 72:00:00)
# -k: keep error (e) and output (o) files generated by the job
#     (they will be saved under your HOME directory)

# If you need to use one or more GPUs,
# change the -q and -l options as follows:
#
# #PBS -q gpu
# #PBS -l select=1:ncpus=48:ngpus=4,walltime=12:00:00
#
#     ngpus: number of GPUs per node (max 4)

# For more info and options:
#
# 1) man qsub (from terminal)
# 2) https://pubs.opengroup.org/onlinepubs/9699919799/utilities/qsub.html

##########################################################################


# You can load any module you need

module load python3 cudnn cuda12.1 proxy

# You can define variables for directories, files, etc
# (do not leave any spaces before and after = )

EXE=run_hybrid_table_lm_finetuning.py

# Move to your pwd (i.e. where you are launching this job)
# (PBS_O_WORKDIR is a PBS-defined variable pointing at pwd)
# (you can get the value of a variable by prepending $ to it)

cd $PBS_O_WORKDIR

# Launch the executable file

CUDA_VISIBLE_DEVICES="0,1,2,3" /davinci-1/home/amozzillo_ext/.conda/envs/turl/bin/python -m torch.distributed.launch --nproc-per-node=4 ./$EXE \
    --output_dir='./output' \
    --model_type=hybrid \
    --model_name_or_path=bert-base-uncased \
    --do_train \
    --data_dir=json_data \
    --evaluate_during_training \
    --mlm \
    --mlm_probability=0.2 \
    --ent_mlm_probability=0.6 \
    --mall_probability=0.7 \
    --per_gpu_train_batch_size=128 \
    --per_gpu_eval_batch_size=64 \
    --gradient_accumulation_steps=1 \
    --learning_rate=1e-4 \
    --num_train_epochs=100 \
    --save_total_limit=10 \
    --seed=1 \
    --cache_dir=cache \
    --overwrite_output_dir \
    --max_entity_candidate=10000 \
    --config_name=configs/table-base-config_v2.json \
    --save_steps=5000 \
    --logging_steps=5000 \
    --use_cand \
    --exclusive_ent=0 \
    --random_sample \
    --linear_scale_lr \
    --allow_tf32 \
    --warmup_epochs=5

sleep 30s