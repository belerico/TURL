{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.executor.memory\", \"8g\"),\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.instances\", \"7\"),\n",
    "        (\"spark.driver.memory\", \"32g\"),\n",
    "        (\"spark.driver.maxResultSize\", \"10g\"),\n",
    "    ]\n",
    ")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ndarray_back(x):\n",
    "    x[\"entityCell\"] = np.array(x[\"entityCell\"])\n",
    "    return x\n",
    "\n",
    "\n",
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/\"\n",
    "train_tables = sc.textFile(data_dir + \"train_tables.jsonl\").map(lambda x: convert_ndarray_back(json.loads(x.strip())))\n",
    "dev_tables = sc.textFile(data_dir + \"dev_tables.jsonl\").map(lambda x: convert_ndarray_back(json.loads(x.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_core_entities_simple(x):\n",
    "    all_entities = []\n",
    "    for i, j in zip(*x[\"entityCell\"].nonzero()):\n",
    "        if j == 0 and j in x[\"entityColumn\"]:\n",
    "            all_entities.append(str(x[\"tableData\"][i][j][\"surfaceLinks\"][0][\"target\"][\"id\"]))\n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_core_entities = train_tables.map(collect_core_entities_simple).filter(lambda x: len(x) >= 2).collect()\n",
    "dev_core_entities = dev_tables.map(collect_core_entities_simple).filter(lambda x: len(x) >= 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    size=312,\n",
    "    alpha=0.025,\n",
    "    window=50,\n",
    "    min_count=1,\n",
    "    max_vocab_size=None,\n",
    "    sample=0,\n",
    "    seed=1,\n",
    "    workers=4,\n",
    "    min_alpha=0.0005,\n",
    "    sg=1,\n",
    "    hs=0,\n",
    "    negative=25,\n",
    "    ns_exponent=0.75,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_core_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../data/dev_result.pkl\", \"rb\") as f:\n",
    "    dev_result = pickle.load(f)\n",
    "\n",
    "dev_dataset = [\n",
    "    [str(item[0].pop()), set([str(z) for z in item[1]]), [str(z) for z in item[7]]] for _, item in dev_result.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError(\"Relevance score length < k\")\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.0\n",
    "    return np.mean(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to log information about training\"\"\"\n",
    "\n",
    "    def __init__(self, dev_dataset):\n",
    "        self.epoch = 0\n",
    "        self.dev_dataset = dev_dataset\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        maps = []\n",
    "        for seed, target, cand in self.dev_dataset:\n",
    "            if seed not in model.wv.vocab:\n",
    "                maps.append(0)\n",
    "            else:\n",
    "                scores = np.full(len(cand), -100.0)\n",
    "                for i, e in enumerate(cand):\n",
    "                    if e in model.wv.vocab:\n",
    "                        scores[i] = model.wv.distance(seed, e)\n",
    "                sorted_scores = scores.argsort()\n",
    "                sorted_labels = [1 if cand[i] in target else 0 for i in sorted_scores]\n",
    "                ap = average_precision(sorted_labels)\n",
    "                maps.append(ap)\n",
    "        print(\"map@dev\", np.mean(maps))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = EpochLogger(dev_dataset)\n",
    "model.train(sentences=dev_core_entities, total_examples=len(dev_core_entities), epochs=100, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_headers = train_tables.map(lambda x: x[\"processed_tableHeaders\"]).collect()\n",
    "dev_headers = dev_tables.map(lambda x: x[\"processed_tableHeaders\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    size=312,\n",
    "    alpha=0.025,\n",
    "    window=50,\n",
    "    min_count=1,\n",
    "    max_vocab_size=None,\n",
    "    sample=0,\n",
    "    seed=1,\n",
    "    workers=4,\n",
    "    min_alpha=0.0005,\n",
    "    sg=1,\n",
    "    hs=0,\n",
    "    negative=25,\n",
    "    ns_exponent=0.75,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeaderEpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to log information about training\"\"\"\n",
    "\n",
    "    def __init__(self, dev_dataset):\n",
    "        self.epoch = 0\n",
    "        self.dev_dataset = dev_dataset\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        maps = []\n",
    "        for headers in self.dev_dataset:\n",
    "            if len(headers) == 1:\n",
    "                continue\n",
    "            seed = headers[0]\n",
    "            target = set(headers[:1])\n",
    "            if seed not in model.wv.vocab:\n",
    "                maps.append(0)\n",
    "            else:\n",
    "                cand = model.wv.most_similar(seed, topn=1000)\n",
    "                sorted_labels = [1 if z[0] in target else 0 for z in cand]\n",
    "                ap = average_precision(sorted_labels)\n",
    "                maps.append(ap)\n",
    "        print(\"map@dev\", np.mean(maps))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_logger = HeaderEpochLogger(dev_headers)\n",
    "model.train(sentences=train_headers, total_examples=len(train_headers), epochs=10, callbacks=[epoch_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "with open(\"../../data/header_vectors.kv\", \"wb\") as f:\n",
    "    model.wv.save(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
