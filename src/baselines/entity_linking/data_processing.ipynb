{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.executor.memory\", \"8g\"),\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.instances\", \"7\"),\n",
    "        (\"spark.driver.memory\", \"150g\"),\n",
    "        (\"spark.driver.maxResultSize\", \"100g\"),\n",
    "    ]\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from operator import add\n",
    "\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_describe(array):\n",
    "    print(\"count\", len(array))\n",
    "    print(\"min\", np.min(array))\n",
    "    print(\"max\", np.max(array))\n",
    "    print(\"mean\", np.mean(array))\n",
    "    print(\"std\", np.std(array))\n",
    "    print(\"10%\", np.percentile(array, 10))\n",
    "    print(\"25%\", np.percentile(array, 25))\n",
    "    print(\"50%\", np.percentile(array, 50))\n",
    "    print(\"60%\", np.percentile(array, 60))\n",
    "    print(\"75%\", np.percentile(array, 75))\n",
    "    print(\"80%\", np.percentile(array, 80))\n",
    "    print(\"90%\", np.percentile(array, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create the index-enwiki dump use this library https://github.com/jcklie/wikimapper\n",
    "wikipedia_wikidata_mapping = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .options(\n",
    "        url=\"jdbc:sqlite:/data/deng.595/workspace/wikimapper/index/index_enwiki-latest.db\",\n",
    "        driver=\"org.sqlite.JDBC\",\n",
    "        dbtable=\"mapping\",\n",
    "    )\n",
    "    .load()\n",
    ")\n",
    "wikipedia_wikidata_mapping.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use dbpedia abstracts and types, so information related to freebase can be ignored\n",
    "wiki_mid_mapping = spark.createDataFrame(\n",
    "    sc.textFile(\"../../../freebase_utils/freebase_dumped/mid2wiki.txt\")\n",
    "    .map(lambda x: x.split())\n",
    "    .map(lambda x: Row(wikipedia_id=int(x[1]), freebase_mid=x[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_types = dict(\n",
    "    spark.createDataFrame(\n",
    "        sc.textFile(\"../../../freebase_utils/dbpedia_2019_08_30/instance_types_en.ttl\")\n",
    "        .map(lambda x: x.split())\n",
    "        .map(\n",
    "            lambda x: Row(\n",
    "                wikipedia_title=unquote(x[0][1:-1]).replace(\"http://dbpedia.org/resource/\", \"\"),\n",
    "                type=x[2][1:-1].split(\"/\")[-1],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")\n",
    "    .rdd.map(lambda x: (x[\"wikidata_id\"], [x[\"type\"]]))\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "print(len(dbpedia_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_abstract = dict(\n",
    "    spark.createDataFrame(\n",
    "        sc.textFile(\"../../../freebase_utils/dbpedia_2019_08_30/short_abstracts_en.ttl\")\n",
    "        .map(lambda x: re.match(r\"(<.+>) (<.+>) (\\\".+\\\")\", x))\n",
    "        .filter(lambda x: x is not None)\n",
    "        .map(\n",
    "            lambda x: Row(\n",
    "                wikipedia_title=unquote(x.group(1)[1:-1]).replace(\"http://dbpedia.org/resource/\", \"\"),\n",
    "                abstract=x.group(3)[1:-1].replace('\\\\\"', '\"'),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")\n",
    "    .rdd.map(lambda x: (x[\"wikidata_id\"], x[\"abstract\"]))\n",
    "    .collect()\n",
    ")\n",
    "print(len(dbpedia_abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_abstract.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_en_types = spark.createDataFrame(\n",
    "    sc.textFile(\"/data/deng.595/workspace/freebase_utils/freebase_dumped/fb_en_types.txt\")\n",
    "    .map(lambda x: x.split(\"\\t\"))\n",
    "    .map(\n",
    "        lambda x: Row(\n",
    "            freebase_mid=x[0],\n",
    "            types=[\n",
    "                z\n",
    "                for z in json.loads(x[1])\n",
    "                if (not z.startswith(\"user.\") and not z.startswith(\"base.\") and not z.startswith(\"common.\"))\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the raw tables\n",
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "train_tables = sc.textFile(data_dir + \"train_tables.jsonl\").map(lambda x: json.loads(x))\n",
    "val_tables = sc.textFile(data_dir + \"dev_tables.jsonl\").map(lambda x: json.loads(x))\n",
    "test_tables = sc.textFile(data_dir + \"test_tables.jsonl\").map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tables.map(lambda x: x[\"_id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(table):\n",
    "    results = []\n",
    "    entity_columns = table.get(\"entityColumn\", [])\n",
    "    entity_cells = np.array(table.get(\"entityCell\", [[]]))\n",
    "    rows = table.get(\"tableData\", {})\n",
    "    num_rows = len(rows)\n",
    "    num_columns = len(rows[0])\n",
    "    entities = set()\n",
    "    for i in range(num_rows):\n",
    "        for j in entity_columns:\n",
    "            if entity_cells[i, j] == 1:\n",
    "                results.append(\n",
    "                    Row(\n",
    "                        table_id=table[\"_id\"],\n",
    "                        table_pgTitle=table[\"pgTitle\"],\n",
    "                        i=i,\n",
    "                        j=j,\n",
    "                        mention=rows[i][j][\"surfaceLinks\"][0][\"surface\"],\n",
    "                        wikipedia_id=rows[i][j][\"surfaceLinks\"][0][\"target\"][\"id\"],\n",
    "                        wikipedia_title=rows[i][j][\"surfaceLinks\"][0][\"target\"][\"title\"],\n",
    "                    )\n",
    "                )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for ours\n",
    "train_mentions = spark.createDataFrame(train_tables.flatMap(get_mentions))\n",
    "val_mentions = spark.createDataFrame(val_tables.flatMap(get_mentions))\n",
    "test_mentions = spark.createDataFrame(test_tables.flatMap(get_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for wikiGS\n",
    "wikipedia_gs_entity_mentions = spark.createDataFrame(\n",
    "    sc.textFile(\"../../data/entity_linking/WikipediaGS_json/entities_instance\")\n",
    "    .map(json.loads)\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            Row(\n",
    "                i=z[2],\n",
    "                tableId=unquote(x[\"tableId\"]),\n",
    "                is_gs=1,\n",
    "                table_pgTitle=unquote(x[\"url\"]).split(\"/\")[-1].replace(\"_\", \" \"),\n",
    "                wikipedia_title=unquote(z[0]).replace(\"http://dbpedia.org/resource/\", \"\"),\n",
    "                mention=z[1],\n",
    "            )\n",
    "            for z in x[\"mappings\"]\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_entity_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gs_mentions(table):\n",
    "    tableId = unquote(table[\"tableId\"])\n",
    "    results = []\n",
    "    rows = table.get(\"contents\", {})\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, cell in enumerate(row):\n",
    "            if \"wikiPageId\" in cell:\n",
    "                results.append(\n",
    "                    Row(tableId=tableId, i=i, j=j, mention=cell[\"data\"], wikipedia_title=unquote(cell[\"wikiPageId\"]))\n",
    "                )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gs_context(table):\n",
    "    x = {}\n",
    "    x[\"tableId\"] = unquote(table[\"tableId\"])\n",
    "    x[\"pgTitle\"] = table.get(\"title\", \"\")\n",
    "    if x[\"pgTitle\"] is None:\n",
    "        x[\"pgTitle\"] = \"\"\n",
    "    else:\n",
    "        x[\"pgTitle\"] = x[\"pgTitle\"].replace(\"- Wikipedia, the free encyclopedia\", \"\")\n",
    "    x[\"sectionTitle\"] = \"\"\n",
    "    x[\"tableCaption\"] = table.get(\"context\", \"\")\n",
    "    if x[\"tableCaption\"] is None:\n",
    "        x[\"tableCaption\"] = \"\"\n",
    "    x[\"tableCaption\"] = x[\"tableCaption\"].replace(\"[edit]\", \"\")\n",
    "    headers = []\n",
    "    for i, row in enumerate(table[\"contents\"]):\n",
    "        for j, cell in enumerate(row):\n",
    "            if len(headers) <= j:\n",
    "                headers.append([\"\"])\n",
    "            if cell.get(\"isHeader\", False):\n",
    "                headers[j].append(cell[\"data\"])\n",
    "    x[\"processed_tableHeaders\"] = [\" \".join(h) for h in headers]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_tables = sc.textFile(\"../../data/entity_linking/WikipediaGS_json/tables_instance\").map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_raw_mentions = spark.createDataFrame(wikipedia_gs_tables.flatMap(get_gs_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_tables = wikipedia_gs_tables.map(get_gs_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_tables.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_raw_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_entity_mentions = wikipedia_gs_entity_mentions.join(\n",
    "    wikipedia_gs_raw_mentions, [\"i\", \"tableId\", \"mention\", \"wikipedia_title\"], \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_mentions.count())\n",
    "train_mentions = train_mentions.join(\n",
    "    wikipedia_gs_entity_mentions.select(\"table_pgTitle\", \"is_gs\").dropDuplicates(), \"table_pgTitle\", \"left\"\n",
    ").where(F.isnull(\"is_gs\"))\n",
    "print(train_mentions.count())\n",
    "print(val_mentions.count())\n",
    "val_mentions = val_mentions.join(\n",
    "    wikipedia_gs_entity_mentions.select(\"table_pgTitle\", \"is_gs\").dropDuplicates(), \"table_pgTitle\", \"left\"\n",
    ").where(F.isnull(\"is_gs\"))\n",
    "print(val_mentions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikipedia_gs_entity_mentions.select(\"wikipedia_title\").dropDuplicates().count())\n",
    "print(dbpedia_types.select(\"wikipedia_title\").dropDuplicates().count())\n",
    "print(\n",
    "    wikipedia_gs_entity_mentions.select(\"wikipedia_title\")\n",
    "    .join(dbpedia_types, \"wikipedia_title\", \"inner\")\n",
    "    .select(\"wikipedia_title\")\n",
    "    .dropDuplicates()\n",
    "    .count()\n",
    ")\n",
    "print(\n",
    "    wikipedia_gs_entity_mentions.select(\"wikipedia_title\")\n",
    "    .join(dbpedia_abstract, \"wikipedia_title\", \"inner\")\n",
    "    .select(\"wikipedia_title\")\n",
    "    .dropDuplicates()\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikipedia_gs_entity_mentions.count())\n",
    "print(wikipedia_gs_entity_mentions.select(\"mention\").dropDuplicates().count())\n",
    "wikipedia_gs_entity_mentions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mentions = sc.textFile(\"../../data/entity_linking/tableMentions.json\").map(json.loads)\n",
    "display(entity_mentions.take(1))\n",
    "display(entity_mentions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mentions_surface = entity_mentions.map(lambda x: (x[\"surfaceForm\"])).distinct()\n",
    "print(entity_mentions_surface.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mentions_surface = wikipedia_gs_entity_mentions.rdd.map(lambda x: x[\"mention\"]).distinct().collect()\n",
    "print(len(entity_mentions_surface))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.oauth2 import service_account\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from multiprocessing import Pool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_lookup(query):\n",
    "    service_url = (\n",
    "        \"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={}&language=en&limit=50&format=json\"\n",
    "    )\n",
    "    url = service_url.format(urllib.parse.quote(query))\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 429 or e.code == 503:\n",
    "                response = e.code\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            else:\n",
    "                response = e.code\n",
    "                break\n",
    "        except urllib.error.URLError as e:\n",
    "            response = None\n",
    "            break\n",
    "        else:\n",
    "            response = json.loads(response.read())\n",
    "            break\n",
    "    #     if isinstance(response, dict):\n",
    "    #         response = [[z.get('id'),z.get('label'),z.get('description')] for z in response.get('search', [])]\n",
    "    return [query, response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_lookup(\"Michael Grant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_candidates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if entity_wikidata_candidates is not None:\n",
    "    i = len(entity_wikidata_candidates)\n",
    "else:\n",
    "    entity_wikidata_candidates = []\n",
    "    i = 0\n",
    "pool = Pool(processes=16)\n",
    "while i < len(entity_mentions_surface):\n",
    "    print(i)\n",
    "    tmp = list(tqdm(pool.imap(wikidata_lookup, entity_mentions_surface[i : i + 10000], chunksize=150), total=10000))\n",
    "    entity_wikidata_candidates.extend(tmp)\n",
    "    i += 10000\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_wikidata_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "entity_mentions_surface_normed_0 = [\n",
    "    re.sub(\"^\\W|\\W$\", \"\", x) for x in entity_mentions_surface if re.sub(\"^\\W|\\W$\", \"\", x) != x\n",
    "]\n",
    "entity_wikidata_candidates_normed_0 = []\n",
    "i = 0\n",
    "pool = Pool(processes=16)\n",
    "while i < len(entity_mentions_surface_normed_0):\n",
    "    print(i)\n",
    "    tmp = list(\n",
    "        tqdm(pool.imap(wikidata_lookup, entity_mentions_surface_normed_0[i : i + 10000], chunksize=300), total=10000)\n",
    "    )\n",
    "    entity_wikidata_candidates_normed_0.extend(tmp)\n",
    "    i += 10000\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_wikidata_candidates = []\n",
    "i = 0\n",
    "pool = Pool(processes=16)\n",
    "while i < len(missing_mentions):\n",
    "    print(i)\n",
    "    tmp = list(tqdm(pool.imap(wikidata_lookup, missing_mentions[i : i + 10000], chunksize=300), total=10000))\n",
    "    missing_wikidata_candidates.extend(tmp)\n",
    "    i += 10000\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_mentions_surface_normed_0) / len(entity_mentions_surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_candidates_normed_0_dict = {\n",
    "    x[0]: x[1] for x in entity_wikidata_candidates_normed_0 if (isinstance(x[1], list) and len(x[1]) != 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(entity_wikidata_candidates):\n",
    "    processed = re.sub(\"^\\W|\\W$\", \"\", x[0])\n",
    "    if processed != x[0] and processed in entity_wikidata_candidates_normed_0_dict:\n",
    "        entity_wikidata_candidates[i][1] += entity_wikidata_candidates_normed_0_dict[processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target = (\n",
    "    spark.createDataFrame(\n",
    "        entity_mentions.map(\n",
    "            lambda x: Row(\n",
    "                id=x[\"_id\"][\"$oid\"],\n",
    "                mention=x[\"surfaceForm\"],\n",
    "                wikipedia_id=x[\"goldAnnotation\"][\"titleId\"],\n",
    "                cell_id=\"%d_%d_%d_%d\" % (x[\"pgId\"], x[\"tableId\"], x[\"cellRow\"], x[\"cellCol\"]),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .join(wikipedia_wikidata_mapping, \"wikipedia_id\", \"inner\")\n",
    "    .join(wiki_mid_mapping, \"wikipedia_id\", \"inner\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target = wikipedia_gs_entity_mentions.join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"left\").join(\n",
    "    wiki_mid_mapping, \"wikipedia_id\", \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=1):\n",
    "    entity_vocab = {}\n",
    "    bad_title = 0\n",
    "    few_entity = 0\n",
    "    with open(os.path.join(data_dir, \"entity_vocab.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            _, entity_id, entity_title, entity_mid, count = line.strip().split(\"\\t\")\n",
    "            if ignore_bad_title and entity_title == \"\":\n",
    "                bad_title += 1\n",
    "            elif int(count) < min_ent_count:\n",
    "                few_entity += 1\n",
    "            else:\n",
    "                entity_vocab[len(entity_vocab)] = {\n",
    "                    \"wiki_id\": int(entity_id),\n",
    "                    \"wiki_title\": entity_title,\n",
    "                    \"mid\": entity_mid,\n",
    "                    \"count\": int(count),\n",
    "                }\n",
    "    print(\n",
    "        \"total number of entity: %d\\nremove because of empty title: %d\\nremove because count<%d: %d\"\n",
    "        % (len(entity_vocab), bad_title, min_ent_count, few_entity)\n",
    "    )\n",
    "    return entity_vocab\n",
    "\n",
    "\n",
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "entity_vocab = load_entity_vocab(data_dir, True, 2)\n",
    "train_all_entities = set([x[\"mid\"] for _, x in entity_vocab.items() if x[\"mid\"] != \"\"])\n",
    "train_all_entities_wiki_id = set([x[\"wiki_id\"] for _, x in entity_vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikipedia_gs_wikidata_candidates.json', \"w\", encoding='utf8') as f:\n",
    "#     json.dump(entity_wikidata_candidates, f)\n",
    "with open(\"wikipedia_gs_wikidata_candidates.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    entity_wikidata_candidates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('wikidata_candidates.json', \"w\", encoding='utf8') as f:\n",
    "#     json.dump(entity_wikidata_candidates, f)\n",
    "with open(\"wikidata_candidates.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    entity_wikidata_candidates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_candidates += missing_wikidata_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_candidates_df = spark.createDataFrame(\n",
    "    sc.parallelize(entity_wikidata_candidates).map(\n",
    "        lambda x: Row(mention=x[0], candidates=x[1] if isinstance(x[1], list) else [])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_entity_mentions_with_candidate = wikipedia_gs_entity_mentions.join(\n",
    "    entity_wikidata_candidates_df, \"mention\", \"left\"\n",
    ").join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mentions_with_candidate = train_mentions.join(entity_wikidata_candidates_df, \"mention\", \"left\").join(\n",
    "    wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\"\n",
    ")\n",
    "val_mentions_with_candidate = val_mentions.join(entity_wikidata_candidates_df, \"mention\", \"left\").join(\n",
    "    wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\"\n",
    ")\n",
    "test_mentions_with_candidate = test_mentions.join(entity_wikidata_candidates_df, \"mention\", \"left\").join(\n",
    "    wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mentions_with_candidate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mentions = val_mentions_with_candidate.where(F.isnull(\"candidates\")).rdd.map(lambda x: x[\"mention\"]).collect()\n",
    "missing_mentions += val_mentions_with_candidate.where(F.isnull(\"candidates\")).rdd.map(lambda x: x[\"mention\"]).collect()\n",
    "missing_mentions = list(set(missing_mentions))\n",
    "print(len(missing_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mentions_with_candidate.dropDuplicates([\"mention\", \"wikidata_id\"]).where(F.size(\"candidates\") != 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target_candidate = entity_wikidata_target.join(entity_wikidata_candidates_df, \"mention\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_mentions_with_candidate.where(F.size(\"candidates\") != 0).count())\n",
    "print(\n",
    "    train_mentions_with_candidate.where(F.size(\"candidates\") != 0)\n",
    "    .join(dbpedia_types.select(\"wikipedia_title\").dropDuplicates(), \"wikipedia_title\", \"inner\")\n",
    "    .count()\n",
    ")\n",
    "print(\n",
    "    train_mentions_with_candidate.where(F.size(\"candidates\") != 0)\n",
    "    .join(dbpedia_abstract.select(\"wikipedia_title\").dropDuplicates(), \"wikipedia_title\", \"inner\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mentions_with_candidate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_for_own(x):\n",
    "    all_processed = []\n",
    "    table_id = x[0]\n",
    "    pgTitle = x[1][1][0]\n",
    "    secTitle = x[1][1][1]\n",
    "    caption = x[1][1][2]\n",
    "    headers = x[1][1][3]\n",
    "    all_entities = x[1][0]\n",
    "    while len(all_entities) > 0:\n",
    "        entities = [[[z[0], z[1]], z[2]] for z in all_entities[:50]]\n",
    "        candidate_entities = {}\n",
    "        for z in all_entities[:50]:\n",
    "            for cand in z[4]:\n",
    "                if cand[0] not in candidate_entities:\n",
    "                    candidate_entities[cand[0]] = [\n",
    "                        len(candidate_entities),\n",
    "                        cand[1],\n",
    "                        cand[2],\n",
    "                        dbpedia_types.get(cand[0], []),\n",
    "                    ]\n",
    "        labels = [candidate_entities[z[3]][0] for z in all_entities[:50]]\n",
    "        cand_for_each = [[candidate_entities[cand[0]][0] for cand in z[4]] for z in all_entities[:50]]\n",
    "        tmp_candidate_entities = [0] * len(candidate_entities)\n",
    "        for k, v in candidate_entities.items():\n",
    "            tmp_candidate_entities[v[0]] = v[1:]\n",
    "        all_processed.append(\n",
    "            [table_id, pgTitle, secTitle, caption, headers, entities, tmp_candidate_entities, labels, cand_for_each]\n",
    "        )\n",
    "        all_entities = all_entities[50:]\n",
    "    return all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_for_own_with_wikidata_id(x):\n",
    "    all_processed = []\n",
    "    table_id = x[0]\n",
    "    pgTitle = x[1][1][0]\n",
    "    secTitle = x[1][1][1]\n",
    "    caption = x[1][1][2]\n",
    "    headers = x[1][1][3]\n",
    "    all_entities = x[1][0]\n",
    "    while len(all_entities) > 0:\n",
    "        entities = [[[z[0], z[1]], z[2]] for z in all_entities[:50]]\n",
    "        candidate_entities = {}\n",
    "        for z in all_entities[:50]:\n",
    "            for cand in z[4]:\n",
    "                if cand[0] not in candidate_entities:\n",
    "                    candidate_entities[cand[0]] = [\n",
    "                        len(candidate_entities),\n",
    "                        cand[1],\n",
    "                        cand[2],\n",
    "                        dbpedia_types.get(cand[0], []),\n",
    "                        cand[0],\n",
    "                    ]\n",
    "        labels = [candidate_entities[z[3]][0] for z in all_entities[:50]]\n",
    "        cand_for_each = [[candidate_entities[cand[0]][0] for cand in z[4]] for z in all_entities[:50]]\n",
    "        tmp_candidate_entities = [0] * len(candidate_entities)\n",
    "        for k, v in candidate_entities.items():\n",
    "            tmp_candidate_entities[v[0]] = v[1:]\n",
    "        all_processed.append(\n",
    "            [table_id, pgTitle, secTitle, caption, headers, entities, tmp_candidate_entities, labels, cand_for_each]\n",
    "        )\n",
    "        all_entities = all_entities[50:]\n",
    "    return all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only output examples with recall>0 for reranking. Including empty candidates or all wrong candidates\n",
    "train_mentions_local = (\n",
    "    train_mentions_with_candidate.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .dropDuplicates([\"mention\", \"wikidata_id\"])\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(\n",
    "        train_tables.map(\n",
    "            lambda x: (x[\"_id\"], [x[\"pgTitle\"], x[\"sectionTitle\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"]])\n",
    "        )\n",
    "    )\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mentions_local = (\n",
    "    val_mentions_with_candidate.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .dropDuplicates([\"mention\", \"wikidata_id\"])\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(\n",
    "        val_tables.map(\n",
    "            lambda x: (x[\"_id\"], [x[\"pgTitle\"], x[\"sectionTitle\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"]])\n",
    "        )\n",
    "    )\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08/20\n",
    "test_mentions_local = (\n",
    "    test_mentions_with_candidate.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(\n",
    "        test_tables.map(\n",
    "            lambda x: (x[\"_id\"], [x[\"pgTitle\"], x[\"sectionTitle\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"]])\n",
    "        )\n",
    "    )\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")\n",
    "print(len(test_mentions_local))\n",
    "test_mentions_local_with_wikidata_id = (\n",
    "    test_mentions_with_candidate.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(\n",
    "        test_tables.map(\n",
    "            lambda x: (x[\"_id\"], [x[\"pgTitle\"], x[\"sectionTitle\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"]])\n",
    "        )\n",
    "    )\n",
    "    .flatMap(build_for_own_with_wikidata_id)\n",
    "    .collect()\n",
    ")\n",
    "print(len(test_mentions_local_with_wikidata_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc.parallelize(test_mentions_local).map(lambda x: x[0]).distinct().count())\n",
    "print(sc.parallelize(test_mentions_local).map(lambda x: len(x[5])).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_mentions_local))\n",
    "print(len(val_mentions_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"train.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(train_mentions_local, f)\n",
    "with open(data_dir + \"dev.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(val_mentions_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"test_own.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(test_mentions_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"test_own_0820.table_entity_linking.with_wikidata_id.json\", \"w\") as f:\n",
    "    json.dump(test_mentions_local_with_wikidata_id, f)\n",
    "with open(data_dir + \"test_own_0820.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(test_mentions_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"train.table_entity_linking.json\", \"r\") as f:\n",
    "    train_mentions_local = sc.parallelize(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mentions_local[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mentions_local = (\n",
    "    wikipedia_gs_entity_mentions_with_candidate.select(\"tableId\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"tableId\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(\n",
    "        wikipedia_gs_tables.map(\n",
    "            lambda x: (x[\"tableId\"], [x[\"pgTitle\"], x[\"sectionTitle\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"]])\n",
    "        )\n",
    "    )\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "with open(data_dir + \"test.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(test_mentions_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_candidate(tables):\n",
    "    results = []\n",
    "    for i, entity in enumerate(tables[5]):\n",
    "        results.append(((tables[0], entity[0][0], entity[0][1]), [tables[7][i], tables[8][i]]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with dumped model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/test_entity_linking_results_2.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    gs_test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/test_own_entity_linking_results_2.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/test_own_0820_entity_linking_results_0.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(result):\n",
    "    result = result[1]\n",
    "    for x in result[1]:\n",
    "        if x in result[0][1]:\n",
    "            if x == result[0][0]:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp = (\n",
    "    sc.parallelize(test_mentions_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(sc.parallelize(test_results).flatMap(lambda x: [((x[0], z[0], z[1]), x[2][i]) for i, z in enumerate(x[1])]))\n",
    "    .map(get_tp)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_dbpedia_types = (\n",
    "    sc.parallelize(train_mentions_local).map(lambda x: set([z for y in x[6] for z in y[2]])).reduce(lambda a, b: a | b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"dbpedia_type_vocab.txt\", \"w\") as f:\n",
    "    f.write(\"{}\\t{}\\n\".format(0, \"[PAD]\"))\n",
    "    for i, t in enumerate(mentioned_dbpedia_types):\n",
    "        f.write(\"{}\\t{}\\n\".format(i + 1, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_mentions = spark.createDataFrame(\n",
    "    entity_wikidata_target_candidate.rdd.filter(\n",
    "        lambda x: x[\"wikidata_id\"] not in [z[0] for z in x[\"candidates\"][:1]]\n",
    "        and x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrong_mentions.where(\"wikipedia_id is not null\").distinct().count())\n",
    "entities = set(\n",
    "    wrong_mentions.where(\"wikipedia_id is not null\").rdd.map(lambda x: x[\"wikipedia_id\"]).distinct().collect()\n",
    ")\n",
    "print(len(entities))\n",
    "print(len(entities & train_all_entities_wiki_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrong_mentions.where(F.size(\"candidates\") != 0).count())\n",
    "print(\n",
    "    wrong_mentions.where(F.size(\"candidates\") != 0)\n",
    "    .join(dbpedia_types.select(\"wikipedia_title\").dropDuplicates(), \"wikipedia_title\", \"inner\")\n",
    "    .count()\n",
    ")\n",
    "print(\n",
    "    wrong_mentions.where(F.size(\"candidates\") != 0)\n",
    "    .join(dbpedia_abstract.select(\"wikipedia_title\").dropDuplicates(), \"wikipedia_title\", \"inner\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_mentions.where(F.size(\"candidates\") != 0).join(dbpedia_types, \"wikipedia_title\", \"left\").where(\n",
    "    \"type is null\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(x, cands):\n",
    "    for i, z in enumerate(cands):\n",
    "        if x == z:\n",
    "            return i\n",
    "    return 999\n",
    "\n",
    "\n",
    "best_recall = (\n",
    "    val_mentions_with_candidate.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .dropDuplicates([\"mention\", \"wikidata_id\"])\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.filter(lambda x: len(x[\"candidates\"]) != 0)\n",
    "    .map(lambda x: get_index(x[\"wikidata_id\"], [z[0] for z in x[\"candidates\"]]))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_describe(best_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_describe(best_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60, 80):\n",
    "    print(i, np.percentile(best_recall, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_gs_entity_mentions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_wikidata_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_wikidata_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mentions_with_candidate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wikidata_all_predicted = test_mentions_with_candidate.where(F.size(\"candidates\") >= 1).count()\n",
    "test_wikidata_TP = (\n",
    "    test_mentions_with_candidate.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]] else 0)\n",
    "    .sum()\n",
    ")\n",
    "test_wikidata_P = test_mentions_with_candidate.count()\n",
    "test_wikidata_best_TP = (\n",
    "    test_mentions_with_candidate.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"]] else 0)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = test_wikidata_TP / test_wikidata_all_predicted\n",
    "recall = test_wikidata_TP / test_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = test_wikidata_best_TP / test_wikidata_all_predicted\n",
    "recall = test_wikidata_best_TP / test_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = our_tp / test_wikidata_all_predicted\n",
    "recall = our_tp / test_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no description\")\n",
    "precision = our_tp / test_wikidata_all_predicted\n",
    "recall = our_tp / test_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no type\")\n",
    "precision = our_tp / test_wikidata_all_predicted\n",
    "recall = our_tp / test_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_wikidata_all_predicted / gs_wikidata_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_wikidata_all_predicted = wikipedia_gs_entity_mentions_with_candidate.where(F.size(\"candidates\") >= 1).count()\n",
    "gs_wikidata_TP = (\n",
    "    wikipedia_gs_entity_mentions_with_candidate.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]] else 0)\n",
    "    .sum()\n",
    ")\n",
    "gs_wikidata_P = wikipedia_gs_entity_mentions_with_candidate.count()\n",
    "gs_wikidata_best_TP = (\n",
    "    wikipedia_gs_entity_mentions_with_candidate.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"]] else 0)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = gs_wikidata_TP / gs_wikidata_all_predicted\n",
    "recall = gs_wikidata_TP / gs_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = gs_wikidata_best_TP / gs_wikidata_all_predicted\n",
    "recall = gs_wikidata_best_TP / gs_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = our_tp / gs_wikidata_all_predicted\n",
    "recall = our_tp / gs_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no description\")\n",
    "precision = our_tp / gs_wikidata_all_predicted\n",
    "recall = our_tp / gs_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no type\")\n",
    "precision = our_tp / gs_wikidata_all_predicted\n",
    "recall = our_tp / gs_wikidata_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp / gs_wikidata_best_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target_candidate.where(F.size(\"candidates\") >= 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recall) / entity_wikidata_target_candidate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_35k_test = spark.createDataFrame(\n",
    "    sc.textFile(\"../../data/entity_linking/35k_test.ids.txt\").map(lambda x: Row(id=x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_35k_test_recall = (\n",
    "    entity_wikidata_target_candidate.join(wiki_35k_test, \"id\", \"inner\")\n",
    "    .rdd.map(lambda x: x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"]])\n",
    "    .collect()\n",
    ")\n",
    "wiki_35k_test_precision = (\n",
    "    entity_wikidata_target_candidate.join(wiki_35k_test, \"id\", \"inner\")\n",
    "    .rdd.map(lambda x: x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(wiki_35k_test_recall) / len(wiki_35k_test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(wiki_35k_test_precision) / sum(wiki_35k_test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_35k_mentions = entity_wikidata_target.join(wiki_35k_test, \"id\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target_candidate = entity_wikidata_target.join(entity_googlekg_candidates_df, \"mention\", \"inner\").join(\n",
    "    entity_wikidata_candidates_df, \"mention\", \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_wikidata_target_candidate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_mentions_surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efthymiou\n",
    "## T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables = sc.wholeTextFiles(\"../../data/efthymiou/t2d/tables_instance_with_context\").map(\n",
    "    lambda x: (x[0].split(\"/\")[-1][:-5], json.loads(x[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.wholeTextFiles(\"../../data/efthymiou/t2d/entities_instance\").map(\n",
    "    lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\"))))\n",
    ").flatMap(lambda x: [y for y in x[1] if len(y) == 3]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities = spark.createDataFrame(\n",
    "    sc.wholeTextFiles(\"../../data/efthymiou/t2d/entities_instance\")\n",
    "    .map(lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\")))))\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            Row(\n",
    "                table_id=x[0],\n",
    "                wikipedia_title=y[0].split(\"/\")[-1],\n",
    "                j=0,\n",
    "                i=int(y[2]),\n",
    "                mention=y[1].replace(\"&nbsp;\", \"\").replace(\"&nbsp\", \"\"),\n",
    "            )\n",
    "            for y in x[1]\n",
    "            if len(y) == 3\n",
    "        ]\n",
    "    )\n",
    ").join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entity_mentions = list(set(t2d_entities.rdd.map(lambda x: x[\"mention\"]).collect()))\n",
    "print(len(t2d_entity_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entity_mentions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_t2d_candidates = []\n",
    "i = 0\n",
    "pool = Pool(processes=16)\n",
    "while i < len(t2d_entity_mentions):\n",
    "    print(i)\n",
    "    tmp = list(tqdm(pool.imap(wikidata_lookup, t2d_entity_mentions[i : i + 10000], chunksize=150), total=10000))\n",
    "    entity_t2d_candidates.extend(tmp)\n",
    "    i += 10000\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_t2d_candidates_df = spark.createDataFrame(\n",
    "    sc.parallelize(entity_t2d_candidates).map(\n",
    "        lambda x: Row(mention=x[0], candidates=x[1] if isinstance(x[1], list) else [])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities_with_candidates = t2d_entities.join(entity_t2d_candidates_df, \"mention\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities_with_candidates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    t2d_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(t2d_tables)\n",
    "    .take(1)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_for_own(x):\n",
    "    all_processed = []\n",
    "    table_id = x[0]\n",
    "    pgTitle = x[1][1][\"pageTitle\"]\n",
    "    secTitle = \"\"\n",
    "    caption = x[1][1][\"title\"]\n",
    "    header_i = x[1][1][\"headerRowIndex\"]\n",
    "    subject_j = x[1][1][\"keyColumnIndex\"]\n",
    "    headers = [column[header_i] for column in x[1][1][\"relation\"][subject_j:]]\n",
    "    all_entities = x[1][0]\n",
    "    total_num = len(all_entities)\n",
    "    chunck_num = int(total_num / max([1, int(total_num / 25)])) + 1\n",
    "    while len(all_entities) > 0:\n",
    "        entities = []\n",
    "        candidate_entities = {}\n",
    "        labels = []\n",
    "        cand_for_each = []\n",
    "        for e in all_entities[:chunck_num]:\n",
    "            row_i = e[0]\n",
    "            e_mention = e[2]\n",
    "            entities.append([[row_i, 0], e_mention])\n",
    "            for cand in e[4]:\n",
    "                if cand[0] not in candidate_entities:\n",
    "                    candidate_entities[cand[0]] = [\n",
    "                        len(candidate_entities),\n",
    "                        cand[1],\n",
    "                        cand[2],\n",
    "                        dbpedia_types.get(cand[0], []),\n",
    "                    ]\n",
    "            labels.append(candidate_entities[e[3]][0])\n",
    "            cand_for_each.append([candidate_entities[cand[0]][0] for cand in e[4]])\n",
    "            for p, column in enumerate(x[1][1][\"relation\"][subject_j + 1 : subject_j + 3]):\n",
    "                if len(column) > row_i:\n",
    "                    e_mention = column[row_i].replace(\"&nbsp;\", \"\").replace(\"&nbsp\", \"\")\n",
    "                    entities.append([[row_i, p + 1], e_mention])\n",
    "                    labels.append(0)\n",
    "                    cand_for_each.append([])\n",
    "\n",
    "        #         entities = [[[z[0],0],z[2]] for z in all_entities[:50]]\n",
    "        #         candidate_entities = {}\n",
    "        #         for z in all_entities[:50]:\n",
    "        #             for cand in z[4]:\n",
    "        #                 if cand[0] not in candidate_entities:\n",
    "        #                     candidate_entities[cand[0]] = [len(candidate_entities),cand[1],cand[2],dbpedia_types.get(cand[0],[])]\n",
    "        #         labels = [candidate_entities[z[3]][0]  for z in all_entities[:50]]\n",
    "        #         cand_for_each = [[candidate_entities[cand[0]][0] for cand in z[4]] for z in all_entities[:50]]\n",
    "        tmp_candidate_entities = [0] * len(candidate_entities)\n",
    "        for k, v in candidate_entities.items():\n",
    "            tmp_candidate_entities[v[0]] = v[1:]\n",
    "        all_processed.append(\n",
    "            [table_id, pgTitle, secTitle, caption, headers, entities, tmp_candidate_entities, labels, cand_for_each]\n",
    "        )\n",
    "        all_entities = all_entities[chunck_num:]\n",
    "    return all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_for_own(sample)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_local = (\n",
    "    t2d_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(t2d_tables)\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_local[70][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_candidate(tables):\n",
    "    results = []\n",
    "    for i, entity in enumerate(tables[5]):\n",
    "        if len(tables[8][i]) == 0:\n",
    "            continue\n",
    "        results.append(((tables[0], entity[0][0], entity[0][1]), [tables[7][i], tables[8][i], tables[6]]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "with open(data_dir + \"t2d.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(t2d_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_all_predicted = t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1).count()\n",
    "t2d_TP = (\n",
    "    t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]] else 0)\n",
    "    .sum()\n",
    ")\n",
    "t2d_P = t2d_entities_with_candidates.count()\n",
    "t2d_best_TP = (\n",
    "    t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"]] else 0)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = t2d_TP / t2d_all_predicted\n",
    "recall = t2d_TP / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = t2d_best_TP / t2d_all_predicted\n",
    "recall = t2d_best_TP / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/t2d_entity_linking_results_0.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(result):\n",
    "    result = result[1]\n",
    "    pred = []\n",
    "    lookup = [result[0][1][0], 0]\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x in result[0][1]:\n",
    "            pred = [x, result[1][1][i]]\n",
    "            break\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x == lookup[0]:\n",
    "            lookup[1] = result[1][1][i]\n",
    "            break\n",
    "    final = pred[0] if pred[0] == lookup[0] or (pred[1] * 0.8) > lookup[1] else lookup[0]\n",
    "    if final == result[0][0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tp(sample_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .map(get_tp)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = our_tp / t2d_all_predicted\n",
    "recall = our_tp / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(result):\n",
    "    result = result[1]\n",
    "    pred = []\n",
    "    lookup = [result[0][1][0], 0]\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x in result[0][1]:\n",
    "            pred = [x, result[1][1][i]]\n",
    "            break\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x == lookup[0]:\n",
    "            lookup[1] = result[1][1][i]\n",
    "            break\n",
    "    final = pred[0] if pred[0] == lookup[0] or (pred[1] * 0.8) > lookup[1] else lookup[0]\n",
    "    if final == result[0][0]:\n",
    "        return (1, result[0][2][final])\n",
    "    else:\n",
    "        return (0, result[0][2][final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0][1][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_results = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .map(lambda x: (x[0], get_tp(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_results = t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1).rdd.map(\n",
    "    lambda x: (\n",
    "        (x[\"table_id\"], x[\"i\"], x[\"j\"]),\n",
    "        (x[\"mention\"], x[\"candidates\"], 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]] else 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = our_results.join(lookup_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = all_results.filter(lambda x: x[1][0][0] == 0 and x[1][1][-1] == 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = all_results.filter(lambda x: x[1][0][0] == 1 and x[1][1][-1] == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(errors))\n",
    "print(len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([x[0][0] for x in errors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([x[0][0] for x in correct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([x[0][0] for x in errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x, t2d_tables_local[x][\"pageTitle\"]] for x in list(set([x[0][0] for x in errors]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in errors if x[0][0] == \"41194422_0_7231546114369966811\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables_local = dict(t2d_tables.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables_local[\"71137051_0_8039724067857124984\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limaye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_tables = sc.wholeTextFiles(\"../../data/efthymiou/LimayeGS/tables_instance\").map(\n",
    "    lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\"))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_tables.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.wholeTextFiles(\"../../data/efthymiou/LimayeGS/entities_instance\").map(\n",
    "    lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\"))))\n",
    ").flatMap(lambda x: [y for y in x[1] if len(y) == 3]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_entities = spark.createDataFrame(\n",
    "    sc.wholeTextFiles(\"../../data/efthymiou/LimayeGS/entities_instance\")\n",
    "    .map(lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\")))))\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            Row(\n",
    "                table_id=x[0],\n",
    "                wikipedia_title=y[0].split(\"/\")[-1],\n",
    "                j=0,\n",
    "                i=int(y[2]),\n",
    "                mention=y[1].replace(\"&nbsp;\", \"\").replace(\"&nbsp\", \"\"),\n",
    "            )\n",
    "            for y in x[1]\n",
    "            if len(y) == 3\n",
    "        ]\n",
    "    )\n",
    ").join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_entity_mentions = list(set(limaye_entities.rdd.map(lambda x: x[\"mention\"]).collect()))\n",
    "print(len(limaye_entity_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_entity_mentions[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_limaye_candidates = []\n",
    "i = 0\n",
    "pool = Pool(processes=16)\n",
    "while i < len(limaye_entity_mentions):\n",
    "    print(i)\n",
    "    tmp = list(tqdm(pool.imap(wikidata_lookup, limaye_entity_mentions[i : i + 10000], chunksize=150), total=10000))\n",
    "    entity_limaye_candidates.extend(tmp)\n",
    "    i += 10000\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_limaye_candidates_df = spark.createDataFrame(\n",
    "    sc.parallelize(entity_limaye_candidates).map(\n",
    "        lambda x: Row(mention=x[0], candidates=x[1] if isinstance(x[1], list) else [])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_entities_with_candidates = limaye_entities.join(entity_limaye_candidates_df, \"mention\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_entities_with_candidates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    limaye_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(limaye_tables)\n",
    "    .take(1)[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_for_own(x):\n",
    "    all_processed = []\n",
    "    table_id = x[0]\n",
    "    pgTitle = \"\"\n",
    "    secTitle = \"\"\n",
    "    caption = \"\"\n",
    "    headers = [\"\" for j in range(len(x[1][1][0]))]\n",
    "    all_entities = x[1][0]\n",
    "    total_num = len(all_entities)\n",
    "    chunck_num = int(total_num / max([1, int(total_num / 25)])) + 1\n",
    "    while len(all_entities) > 0:\n",
    "        entities = []\n",
    "        candidate_entities = {}\n",
    "        labels = []\n",
    "        cand_for_each = []\n",
    "        for e in all_entities[:chunck_num]:\n",
    "            row_i = e[0]\n",
    "            e_mention = e[2]\n",
    "            entities.append([[row_i, 0], e_mention])\n",
    "            for cand in e[4]:\n",
    "                if cand[0] not in candidate_entities:\n",
    "                    candidate_entities[cand[0]] = [\n",
    "                        len(candidate_entities),\n",
    "                        cand[1],\n",
    "                        cand[2],\n",
    "                        dbpedia_types.get(cand[0], []),\n",
    "                    ]\n",
    "            labels.append(candidate_entities[e[3]][0])\n",
    "            cand_for_each.append([candidate_entities[cand[0]][0] for cand in e[4]])\n",
    "            for p, cell in enumerate(x[1][1][row_i][1:]):\n",
    "                e_mention = cell\n",
    "                if e_mention != \"\":\n",
    "                    entities.append([[row_i, p + 1], e_mention])\n",
    "                    labels.append(0)\n",
    "                    cand_for_each.append([])\n",
    "\n",
    "        #         entities = [[[z[0],0],z[2]] for z in all_entities[:50]]\n",
    "        #         candidate_entities = {}\n",
    "        #         for z in all_entities[:50]:\n",
    "        #             for cand in z[4]:\n",
    "        #                 if cand[0] not in candidate_entities:\n",
    "        #                     candidate_entities[cand[0]] = [len(candidate_entities),cand[1],cand[2],dbpedia_types.get(cand[0],[])]\n",
    "        #         labels = [candidate_entities[z[3]][0]  for z in all_entities[:50]]\n",
    "        #         cand_for_each = [[candidate_entities[cand[0]][0] for cand in z[4]] for z in all_entities[:50]]\n",
    "        tmp_candidate_entities = [0] * len(candidate_entities)\n",
    "        for k, v in candidate_entities.items():\n",
    "            tmp_candidate_entities[v[0]] = v[1:]\n",
    "        all_processed.append(\n",
    "            [table_id, pgTitle, secTitle, caption, headers, entities, tmp_candidate_entities, labels, cand_for_each]\n",
    "        )\n",
    "        all_entities = all_entities[chunck_num:]\n",
    "    return all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_for_own(sample)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_local = (\n",
    "    limaye_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z[0] for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(limaye_tables)\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_local[70][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_candidate(tables):\n",
    "    results = []\n",
    "    for i, entity in enumerate(tables[5]):\n",
    "        if len(tables[8][i]) == 0:\n",
    "            continue\n",
    "        results.append(((tables[0], entity[0][0], entity[0][1]), [tables[7][i], tables[8][i], tables[6]]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "with open(data_dir + \"limaye.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(limaye_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limaye_all_predicted = limaye_entities_with_candidates.where(F.size(\"candidates\") >= 1).count()\n",
    "limaye_TP = (\n",
    "    limaye_entities_with_candidates.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"][:1]] else 0)\n",
    "    .sum()\n",
    ")\n",
    "limaye_P = limaye_entities_with_candidates.count()\n",
    "limaye_best_TP = (\n",
    "    limaye_entities_with_candidates.where(F.size(\"candidates\") >= 1)\n",
    "    .rdd.map(lambda x: 1 if x[\"wikidata_id\"] in [z[0] for z in x[\"candidates\"]] else 0)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = limaye_TP / limaye_all_predicted\n",
    "recall = limaye_TP / limaye_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/limaye_entity_linking_results_0.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tp = (\n",
    "    sc.parallelize(limaye_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(sc.parallelize(test_results).flatMap(lambda x: [((x[0], z[0], z[1]), x[2][i]) for i, z in enumerate(x[1])]))\n",
    "    .map(get_tp)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = our_tp / limaye_all_predicted\n",
    "recall = our_tp / limaye_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
