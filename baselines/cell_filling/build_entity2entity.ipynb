{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.executor.memory\", \"8g\"),\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.instances\", \"7\"),\n",
    "        (\"spark.driver.memory\", \"32g\"),\n",
    "        (\"spark.driver.maxResultSize\", \"10g\"),\n",
    "    ]\n",
    ")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ndarray_back(x):\n",
    "    x[\"entityCell\"] = np.array(x[\"entityCell\"])\n",
    "    return x\n",
    "\n",
    "\n",
    "data_dir = \"../../data/\"\n",
    "train_tables = sc.textFile(data_dir + \"train_tables.jsonl\").map(lambda x: convert_ndarray_back(json.loads(x.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_entity(x):\n",
    "    entities = []\n",
    "    valid_rows = set()\n",
    "    for i, j in zip(*x[\"entityCell\"].nonzero()):\n",
    "        entities.append(\n",
    "            Row(\n",
    "                t_id=x[\"_id\"],\n",
    "                entity=x[\"tableData\"][i][j][\"surfaceLinks\"][0][\"target\"][\"id\"],\n",
    "                c_id=int(j),\n",
    "                c_name=x[\"processed_tableHeaders\"][j],\n",
    "                r_id=int(i),\n",
    "            )\n",
    "        )\n",
    "        valid_rows.add(i)\n",
    "    #     for i in valid_rows:\n",
    "    #         if x['pgId']!=-1:\n",
    "    #             entities.append(Row(\n",
    "    #                 t_id=x[\"_id\"],\n",
    "    #                 entity=x['pgId'],\n",
    "    #                 c_id=-1,\n",
    "    #                 c_name='[TITLE]',\n",
    "    #                 r_id=int(i)\n",
    "    #             ))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = spark.createDataFrame(train_tables.flatMap(get_table_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_e2e = entity_df.selectExpr('r_id','t_id','entity as e1').filter(F.col('c_id')==0).join(\\\n",
    "                entity_df.withColumnRenamed('entity','e2').filter(F.col('c_id')!=0),\\\n",
    "                ['r_id','t_id'],'inner')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_e2e.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2h = (\n",
    "    row_e2e.selectExpr(\"t_id as t1\", \"e1\", \"e2\", \"c_name as h1\")\n",
    "    .join(row_e2e.selectExpr(\"t_id as t2\", \"e1\", \"e2\", \"c_name as h2\"), [\"e1\", \"e2\"], \"inner\")\n",
    "    .select(\"t1\", \"t2\", \"h1\", \"h2\")\n",
    "    .dropDuplicates()\n",
    "    .groupBy([\"h1\", \"h2\"])\n",
    "    .agg(F.count(\"t1\").alias(\"count\"))\n",
    "    .filter(F.col(\"h1\") != F.col(\"h2\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/n_h2h.pkl\", \"wb\") as f:\n",
    "    h2h_local = {}\n",
    "    for h1, h2, count in h2h.rdd.map(lambda x: (x[\"h1\"], x[\"h2\"], x[\"count\"])).collect():\n",
    "        if h1 not in h2h_local:\n",
    "            h2h_local[h1] = {}\n",
    "        h2h_local[h1][h2] = count\n",
    "    pickle.dump(h2h_local, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_e2e = row_e2e.rdd.map(lambda x: (x[\"e1\"], [[x[\"t_id\"], x[\"c_id\"], x[\"c_name\"], x[\"e2\"]]])).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/e2e_row.json\", \"w\") as f:\n",
    "    json.dump(dict(row_e2e.collect()), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_e2e.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2column = entity_df.rdd.map(lambda x: (x[\"entity\"], [[x[\"t_id\"], x[\"c_id\"]]])).reduceByKey(add)\n",
    "table_column2e = entity_df.rdd.map(lambda x: (\"%s-%d\" % (x[\"t_id\"], x[\"c_id\"]), [x[\"entity\"]])).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/e2column.json\", \"w\") as f:\n",
    "    json.dump(dict(e2column.collect()), f, indent=2)\n",
    "with open(\"../../data/table_column2e\", \"w\") as f:\n",
    "    json.dump(dict(table_column2e.collect()), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_column2e.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2column.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
