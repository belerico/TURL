{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.executor.memory\", \"8g\"),\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.instances\", \"7\"),\n",
    "        (\"spark.driver.memory\", \"32g\"),\n",
    "        (\"spark.driver.maxResultSize\", \"10g\"),\n",
    "    ]\n",
    ")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ndarray_back(x):\n",
    "    x[\"entityCell\"] = np.array(x[\"entityCell\"])\n",
    "    return x\n",
    "\n",
    "\n",
    "data_dir = \"../../data/\"\n",
    "train_tables = sc.textFile(data_dir + \"train_tables.jsonl\").map(lambda x: convert_ndarray_back(json.loads(x.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_entity_caption_label(x):\n",
    "    core_entities = set()\n",
    "    for i, j in zip(*x[\"entityCell\"].nonzero()):\n",
    "        if j == 0 and j in x[\"entityColumn\"]:\n",
    "            core_entities.add(x[\"tableData\"][i][j][\"surfaceLinks\"][0][\"target\"][\"id\"])\n",
    "    return list(core_entities), x[\"_id\"], x[\"tableCaption\"], x[\"processed_tableHeaders\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rdd = train_tables.map(get_core_entity_caption_label)\n",
    "entity_rdd = table_rdd.flatMap(lambda x: [(z, x[1], x[2], x[3]) for z in x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = spark.createDataFrame(table_rdd, [\"entities\", \"table_id\", \"caption\", \"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_tokenizer = Tokenizer(inputCol=\"caption\", outputCol=\"caption_term\")\n",
    "header_tokenizer = Tokenizer(inputCol=\"header\", outputCol=\"header_term\")\n",
    "list_stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "caption_remover = StopWordsRemover(inputCol=\"caption_term\", outputCol=\"caption_term_cleaned\")\n",
    "header_remover = StopWordsRemover(inputCol=\"header_term\", outputCol=\"header_term_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df_tokenizered = header_remover.transform(\n",
    "    header_tokenizer.transform(caption_remover.transform(caption_tokenizer.transform(table_df)))\n",
    ").select(\"entities\", \"table_id\", \"caption_term_cleaned\", \"header_term_cleaned\", \"header\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df_tokenizered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_term_freq = (\n",
    "    table_df_tokenizered.select(\"caption_term_cleaned\")\n",
    "    .rdd.flatMap(lambda x: [(z, 1) for z in x[\"caption_term_cleaned\"]])\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "header_term_freq = (\n",
    "    table_df_tokenizered.select(\"header_term_cleaned\")\n",
    "    .rdd.flatMap(lambda x: [(z, 1) for z in x[\"header_term_cleaned\"]])\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "header_freq = table_df_tokenizered.select(\"header\").rdd.map(lambda x: (x[\"header\"], 1)).reduceByKey(add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(header_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = table_df_tokenizered.select(\n",
    "    F.explode(\"entities\").alias(\"entity\"), \"table_id\", \"caption_term_cleaned\", \"header_term_cleaned\", \"header\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_caption_term_freq = (\n",
    "    entity_df.select(\"entity\", \"caption_term_cleaned\")\n",
    "    .rdd.flatMap(lambda x: [((x[\"entity\"], z), 1) for z in x[\"caption_term_cleaned\"]])\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda x: (x[0][0], [(x[0][1], x[1])]))\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "entity_header_term_freq = (\n",
    "    entity_df.select(\"entity\", \"header_term_cleaned\")\n",
    "    .rdd.flatMap(lambda x: [((x[\"entity\"], z), 1) for z in x[\"header_term_cleaned\"]])\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda x: (x[0][0], [(x[0][1], x[1])]))\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "entity_header_freq = (\n",
    "    entity_df.select(\"entity\", \"header\")\n",
    "    .rdd.map(lambda x: ((x[\"entity\"], x[\"header\"]), 1))\n",
    "    .reduceByKey(add)\n",
    "    .map(lambda x: (x[0][0], [(x[0][1], x[1])]))\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tables = (\n",
    "    entity_df.select(\"entity\", \"table_id\")\n",
    "    .groupBy(\"entity\")\n",
    "    .agg(F.collect_list(\"table_id\").alias(\"tables\"))\n",
    "    .rdd.map(lambda x: (x[\"entity\"], x[\"tables\"]))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/entity_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_tables, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in entity_header_freq:\n",
    "    entity_header_freq[e] = [sum([count for _, count in entity_header_freq[e]]), dict(entity_header_freq[e])]\n",
    "\n",
    "with open(\"../../data/entity_header_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_header_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_header_term_freq = dict(entity_header_term_freq)\n",
    "for e in entity_header_term_freq:\n",
    "    entity_header_term_freq[e] = [\n",
    "        sum([count for _, count in entity_header_term_freq[e]]),\n",
    "        dict(entity_header_term_freq[e]),\n",
    "    ]\n",
    "\n",
    "with open(\"../../data/entity_header_term_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_header_term_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_caption_term_freq = dict(entity_caption_term_freq)\n",
    "for e in entity_caption_term_freq:\n",
    "    entity_caption_term_freq[e] = [\n",
    "        sum([count for _, count in entity_caption_term_freq[e]]),\n",
    "        dict(entity_caption_term_freq[e]),\n",
    "    ]\n",
    "\n",
    "with open(\"../../data/entity_caption_term_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump(entity_caption_term_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_term_freq = dict(caption_term_freq)\n",
    "with open(\"../../data/caption_term_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump([sum([count for _, count in caption_term_freq.items()]), caption_term_freq], f)\n",
    "\n",
    "header_term_freq = dict(header_term_freq)\n",
    "with open(\"../../data/header_term_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump([sum([count for _, count in header_term_freq.items()]), header_term_freq], f)\n",
    "\n",
    "header_freq = dict(header_freq)\n",
    "with open(\"../../data/header_freq.pkl\", \"wb\") as f:\n",
    "    pickle.dump([sum([count for _, count in header_freq.items()]), header_freq], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in entity_tables:\n",
    "    if len(entity_tables[e]) != sum([count for _, count in entity_header_freq[e]]):\n",
    "        print(e, len(entity_tables[e]), sum([count for _, count in entity_header_freq[e]]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_term_freq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_header_freq[1677]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_rdd.filter(lambda x: x[0] == 5839439).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/dev_result.pkl\", \"rb\") as f:\n",
    "    dev_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=1):\n",
    "    entity_vocab = {}\n",
    "    bad_title = 0\n",
    "    few_entity = 0\n",
    "    with open(os.path.join(data_dir, \"entity_vocab.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            _, entity_id, entity_title, entity_mid, count = line.strip().split(\"\\t\")\n",
    "            if ignore_bad_title and entity_title == \"\":\n",
    "                bad_title += 1\n",
    "            elif int(count) < min_ent_count:\n",
    "                few_entity += 1\n",
    "            else:\n",
    "                entity_vocab[len(entity_vocab)] = {\n",
    "                    \"wiki_id\": int(entity_id),\n",
    "                    \"wiki_title\": entity_title,\n",
    "                    \"mid\": entity_mid,\n",
    "                    \"count\": int(count),\n",
    "                }\n",
    "    print(\n",
    "        \"total number of entity: %d\\nremove because of empty title: %d\\nremove because count<%d: %d\"\n",
    "        % (len(entity_vocab), bad_title, min_ent_count, few_entity)\n",
    "    )\n",
    "    return entity_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_vocab = load_entity_vocab(\"../../data\", True, 2)\n",
    "train_all_entities = set([x[\"wiki_id\"] for _, x in entity_vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_final = {}\n",
    "for id, result in dev_result.items():\n",
    "    _, target_entities, pneural, pall, pee, pce, ple, cand_e, cand_c = result\n",
    "    target_entities = set(target_entities)\n",
    "    cand_e = set([e for e in cand_e if e in train_all_entities])\n",
    "    cand_c = set([e for e in cand_c if e in train_all_entities])\n",
    "    cand_all = set([e for e in cand_c | cand_e if e in train_all_entities])\n",
    "    recall_e = len(cand_e & target_entities) / len(target_entities)\n",
    "    recall_c = len(cand_c & target_entities) / len(target_entities)\n",
    "    recall_all = len(cand_all & target_entities) / len(target_entities)\n",
    "\n",
    "    ranked_neural = sorted(pneural.items(), key=lambda z: z[1] + 30 * pee[z[0]], reverse=True)\n",
    "    ranked_neural = [1 if z[0] in target_entities else 0 for z in ranked_neural if z[0] in train_all_entities]\n",
    "    ap_neural = average_precision(ranked_neural)\n",
    "\n",
    "    ranked_all = sorted(pall.items(), key=lambda z: 100 * pee[z[0]] + 1 * pce[z[0]] + 0.5 * ple[z[0]], reverse=True)\n",
    "    ranked_all = [1 if z[0] in target_entities else 0 for z in ranked_all if z[0] in train_all_entities]\n",
    "    ap_all = average_precision(ranked_all)\n",
    "\n",
    "    #     ranked_e = sorted(pee.items(),key=lambda z:z[1],reverse=True)\n",
    "    #     ranked_e = [1 if z[0] in target_entities else 0 for z in ranked_e if z[0] in train_all_entities]\n",
    "    #     assert len(ranked_e) == len(ranked_neural)\n",
    "    #     ap_e = average_precision(ranked_e)\n",
    "\n",
    "    #     ranked_c = sorted(pce.items(),key=lambda z:z[1],reverse=True)\n",
    "    #     ap_c = average_precision([1 if z[0] in target_entities else 0 for z in ranked_c if z[0] in train_all_entities])\n",
    "\n",
    "    #     ranked_l = sorted(ple.items(),key=lambda z:z[1],reverse=True)\n",
    "    #     ap_l = average_precision([1 if z[0] in target_entities else 0 for z in ranked_l if z[0] in train_all_entities])\n",
    "\n",
    "    dev_final[id] = [recall_all, recall_e, recall_c, ap_neural, ap_all, ap_e, ap_c, ap_l]\n",
    "\n",
    "for i in range(8):\n",
    "    print(np.mean([z[i] for _, z in dev_final.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_result[\"13591903-1\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for z in dev_final if z[4] >= z[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, z[3], z[4], z[5]) for i, z in dev_final.items() if z[4] >= z[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
