{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import WEIGHTS_NAME, AdamW, BertConfig, BertTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "from src.baselines.cell_filling.cell_filling import *\n",
    "from src.baselines.row_population.metric import average_precision, ndcg_at_k\n",
    "from src.data_loader.ct_wiki_data_loaders import *\n",
    "from src.data_loader.el_data_loaders import *\n",
    "from src.data_loader.header_data_loaders import *\n",
    "from src.data_loader.hybrid_data_loaders import *\n",
    "from src.data_loader.re_data_loaders import *\n",
    "from src.model import metric\n",
    "from src.model.configuration import TableConfig\n",
    "from src.model.model import (\n",
    "    BertRE,\n",
    "    HybridTableCER,\n",
    "    HybridTableCT,\n",
    "    HybridTableEL,\n",
    "    HybridTableMaskedLM,\n",
    "    HybridTableRE,\n",
    "    TableHeaderRanking,\n",
    ")\n",
    "from src.utils.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"CER\": (TableConfig, HybridTableCER, BertTokenizer),\n",
    "    \"CF\": (TableConfig, HybridTableMaskedLM, BertTokenizer),\n",
    "    \"HR\": (TableConfig, TableHeaderRanking, BertTokenizer),\n",
    "    \"CT\": (TableConfig, HybridTableCT, BertTokenizer),\n",
    "    \"EL\": (TableConfig, HybridTableEL, BertTokenizer),\n",
    "    \"RE\": (TableConfig, HybridTableRE, BertTokenizer),\n",
    "    \"REBERT\": (BertConfig, BertRE, BertTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory, this will be used to load test data\n",
    "data_dir = \"~/turl-data\"\n",
    "data_dir = os.path.expanduser(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = \"/home/fbelotti/projects/TURL/src/configs/table-base-config_v2.json\"\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "entity_wikid2id = {entity_vocab[x][\"wiki_id\"]: x for x in entity_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "This notebook shows examples of how to using the model components and running evaluation of different tasks.\n",
    "* [Pretrained and Cell Filling](#cf)\n",
    "* [Entity Linking](#el)\n",
    "* [Column Type Classification](#ct)\n",
    "* [Relation Extraction](#re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cf\"></a>\n",
    "# Pretrained and CF\n",
    "Here we show how to use the pretrained model to get contextualized representation for a given input table. \n",
    "\n",
    "We use the cell filling task for demonstration as it does not need task-specific finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, _ = MODEL_CLASSES[\"CF\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True\n",
    "\n",
    "# For CF, we use the base HybridTableMaskedLM, and directly load the pretrained checkpoint\n",
    "checkpoint = \"output/hybrid/v2/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/\"\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(os.path.join(checkpoint, \"pytorch_model.bin\"))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# load the module for cell filling baselines\n",
    "CF = cell_filling(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"CF_test_data.json\"), \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "print(\"example for cell filling\")\n",
    "display(dev_data[0])\n",
    "# the dataset here is the dataloader for pretraining. We use it to pass the config to construct the cell filling example\n",
    "dataset = WikiHybridTableDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    max_cell=100,\n",
    "    max_input_tok=350,\n",
    "    max_input_ent=150,\n",
    "    src=\"dev\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    "    mode=0,\n",
    ")\n",
    "print(\"example of pretraining data\")\n",
    "with open(os.path.join(data_dir, \"dev_tables.jsonl\"), \"r\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        break\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of converting an arbitrary table to input\n",
    "# Here we show an example for cell filling task\n",
    "# The input entites are entities in the subject column, we append [ENT_MASK] and use its representation to match with the candidate entities\n",
    "def CF_build_input(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, entity_cand, config):\n",
    "    tokenized_pgTitle = config.tokenizer.encode(pgTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_meta = tokenized_pgTitle + config.tokenizer.encode(\n",
    "        secTitle, max_length=config.max_title_length, add_special_tokens=False\n",
    "    )\n",
    "    if caption != secTitle:\n",
    "        tokenized_meta += config.tokenizer.encode(caption, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_headers = [\n",
    "        config.tokenizer.encode(header, max_length=config.max_header_length, add_special_tokens=False)\n",
    "        for header in headers\n",
    "    ]\n",
    "    input_tok = []\n",
    "    input_tok_pos = []\n",
    "    input_tok_type = []\n",
    "    tokenized_meta_length = len(tokenized_meta)\n",
    "    input_tok += tokenized_meta\n",
    "    input_tok_pos += list(range(tokenized_meta_length))\n",
    "    input_tok_type += [0] * tokenized_meta_length\n",
    "    header_span = []\n",
    "    for tokenized_header in tokenized_headers:\n",
    "        tokenized_header_length = len(tokenized_header)\n",
    "        header_span.append([len(input_tok), len(input_tok) + tokenized_header_length])\n",
    "        input_tok += tokenized_header\n",
    "        input_tok_pos += list(range(tokenized_header_length))\n",
    "        input_tok_type += [1] * tokenized_header_length\n",
    "\n",
    "    input_ent = [config.entity_wikid2id[pgEnt] if pgEnt != -1 else 0]\n",
    "    input_ent_text = [tokenized_pgTitle[: config.max_cell_length]]\n",
    "    input_ent_type = [2]\n",
    "\n",
    "    # core entities in the subject column\n",
    "    input_ent += [config.entity_wikid2id[entity] for entity in core_entities]\n",
    "    input_ent_text += [\n",
    "        (\n",
    "            config.tokenizer.encode(entity_text, max_length=config.max_cell_length, add_special_tokens=False)\n",
    "            if len(entity_text) != 0\n",
    "            else []\n",
    "        )\n",
    "        for entity_text in core_entities_text\n",
    "    ]\n",
    "    input_ent_type += [3] * len(core_entities)\n",
    "\n",
    "    # append [ent_mask]\n",
    "    input_ent += [config.entity_wikid2id[\"[ENT_MASK]\"]] * len(core_entities)\n",
    "    input_ent_text += [[]] * len(core_entities)\n",
    "    input_ent_type += [4] * len(core_entities)\n",
    "\n",
    "    input_ent_cell_length = [len(x) if len(x) != 0 else 1 for x in input_ent_text]\n",
    "    max_cell_length = max(input_ent_cell_length)\n",
    "    input_ent_text_padded = np.zeros([len(input_ent_text), max_cell_length], dtype=int)\n",
    "    for i, x in enumerate(input_ent_text):\n",
    "        input_ent_text_padded[i, : len(x)] = x\n",
    "    assert len(input_ent) == 1 + 2 * len(core_entities)\n",
    "\n",
    "    input_tok_mask = np.ones([1, len(input_tok), len(input_tok) + len(input_ent)], dtype=int)\n",
    "    input_tok_mask[0, header_span[0][0] : header_span[0][1], len(input_tok) + 1 + len(core_entities) :] = 0\n",
    "    input_tok_mask[\n",
    "        0, header_span[1][0] : header_span[1][1], len(input_tok) + 1 : len(input_tok) + 1 + len(core_entities)\n",
    "    ] = 0\n",
    "    input_tok_mask[0, :, len(input_tok) + 1 + len(core_entities) :] = 0\n",
    "\n",
    "    # build the mask for entities\n",
    "    input_ent_mask = np.ones([1, len(input_ent), len(input_tok) + len(input_ent)], dtype=int)\n",
    "    input_ent_mask[0, 1 : 1 + len(core_entities), header_span[1][0] : header_span[1][1]] = 0\n",
    "    input_ent_mask[0, 1 : 1 + len(core_entities), len(input_tok) + 1 + len(core_entities) :] = np.eye(\n",
    "        len(core_entities), dtype=int\n",
    "    )\n",
    "    input_ent_mask[0, 1 + len(core_entities) :, header_span[0][0] : header_span[0][1]] = 0\n",
    "    input_ent_mask[0, 1 + len(core_entities) :, len(input_tok) + 1 : len(input_tok) + 1 + len(core_entities)] = np.eye(\n",
    "        len(core_entities), dtype=int\n",
    "    )\n",
    "    input_ent_mask[0, 1 + len(core_entities) :, len(input_tok) + 1 + len(core_entities) :] = np.eye(\n",
    "        len(core_entities), dtype=int\n",
    "    )\n",
    "\n",
    "    input_tok_mask = torch.LongTensor(input_tok_mask)\n",
    "    input_ent_mask = torch.LongTensor(input_ent_mask)\n",
    "\n",
    "    input_tok = torch.LongTensor([input_tok])\n",
    "    input_tok_type = torch.LongTensor([input_tok_type])\n",
    "    input_tok_pos = torch.LongTensor([input_tok_pos])\n",
    "\n",
    "    input_ent = torch.LongTensor([input_ent])\n",
    "    input_ent_text = torch.LongTensor([input_ent_text_padded])\n",
    "    input_ent_cell_length = torch.LongTensor([input_ent_cell_length])\n",
    "    input_ent_type = torch.LongTensor([input_ent_type])\n",
    "\n",
    "    input_ent_mask_type = torch.zeros_like(input_ent)\n",
    "    input_ent_mask_type[:, 1 + len(core_entities) :] = config.entity_wikid2id[\"[ENT_MASK]\"]\n",
    "\n",
    "    candidate_entity_set = [config.entity_wikid2id[entity] for entity in entity_cand]\n",
    "    candidate_entity_set = torch.LongTensor([candidate_entity_set])\n",
    "\n",
    "    return (\n",
    "        input_tok,\n",
    "        input_tok_type,\n",
    "        input_tok_pos,\n",
    "        input_tok_mask,\n",
    "        input_ent,\n",
    "        input_ent_text,\n",
    "        input_ent_cell_length,\n",
    "        input_ent_type,\n",
    "        input_ent_mask_type,\n",
    "        input_ent_mask,\n",
    "        candidate_entity_set,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for table_id, pgEnt, pgTitle, secTitle, caption, (h1, h2), data_sample in tqdm(dev_data):\n",
    "    result = []\n",
    "    while len(data_sample) != 0:\n",
    "        core_entities = []\n",
    "        core_entities_text = []\n",
    "        target_entities = []\n",
    "        all_entity_cand = set()\n",
    "        entity_cand = []\n",
    "        for (core_e, core_e_text), target_e in data_sample[:100]:\n",
    "            assert target_e in entity_wikid2id\n",
    "            core_entities.append(core_e)\n",
    "            core_entities_text.append(core_e_text)\n",
    "            target_entities.append(target_e)\n",
    "            cands = CF.get_cand_row(core_e, h2)\n",
    "            cands = {key: value for key, value in cands.items() if key in entity_wikid2id}\n",
    "            entity_cand.append(cands)\n",
    "            all_entity_cand |= set(cands.keys())\n",
    "        all_entity_cand = list(all_entity_cand)\n",
    "        (\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_tok_mask,\n",
    "            input_ent,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent_type,\n",
    "            input_ent_mask_type,\n",
    "            input_ent_mask,\n",
    "            candidate_entity_set,\n",
    "        ) = CF_build_input(\n",
    "            pgEnt, pgTitle, secTitle, caption, [h1, h2], core_entities, core_entities_text, all_entity_cand, dataset\n",
    "        )\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask_type = input_ent_mask_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        candidate_entity_set = candidate_entity_set.to(device)\n",
    "        with torch.no_grad():\n",
    "            tok_outputs, ent_outputs = model(\n",
    "                input_tok,\n",
    "                input_tok_type,\n",
    "                input_tok_pos,\n",
    "                input_tok_mask,\n",
    "                input_ent_text,\n",
    "                input_ent_text_length,\n",
    "                input_ent_mask_type,\n",
    "                input_ent,\n",
    "                input_ent_type,\n",
    "                input_ent_mask,\n",
    "                candidate_entity_set,\n",
    "            )\n",
    "            num_sample = len(target_entities)\n",
    "            ent_prediction_scores = ent_outputs[0][0, num_sample + 1 :].tolist()\n",
    "        for i, target_e in enumerate(target_entities):\n",
    "            predictions = ent_prediction_scores[i]\n",
    "            if len(entity_cand[i]) == 0:\n",
    "                result.append([target_e, entity_cand[i], [], []])\n",
    "            else:\n",
    "                tmp_cand_scores = []\n",
    "                for j, cand_e in enumerate(all_entity_cand):\n",
    "                    if cand_e in entity_cand[i]:\n",
    "                        tmp_cand_scores.append([cand_e, predictions[j]])\n",
    "                sorted_cand_scores = sorted(tmp_cand_scores, key=lambda z: z[1], reverse=True)\n",
    "                sorted_cands = [z[0] for z in sorted_cand_scores]\n",
    "                # use H2H as baseline\n",
    "                base_sorted_cands = CF.rank_cand_h2h(h2, entity_cand[i])\n",
    "                result.append([target_e, entity_cand[i], sorted_cands, base_sorted_cands])\n",
    "        data_sample = data_sample[100:]\n",
    "    results.append(\n",
    "        {\"pgTitle\": pgTitle, \"secTitle\": secTitle, \"caption\": caption, \"headers\": [h1, h2], \"result\": result}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tok(metadata) outputs\", len(tok_outputs))\n",
    "print(\"tok prediction logits: [batch_size, num_toks, vocab_size]\\n\", tok_outputs[0].shape)\n",
    "print(\"tok hidden states: [batch_size, num_toks, hidden_size]\\n\", tok_outputs[1].shape)\n",
    "print(\n",
    "    \"tok attention: n_layers*[batch_size, num_attention_headers, num_toks, num_toks+num_ents]\\n\",\n",
    "    tok_outputs[2][0].shape,\n",
    ")\n",
    "print(\"entity(cell) outputs\", len(ent_outputs))\n",
    "print(\"ent prediction logits: [batch_size, num_ents, candidate_size]\\n\", ent_outputs[0].shape)\n",
    "print(\"ent hidden states: [batch_size, num_ents, hidden_size]\\n\", ent_outputs[1].shape)\n",
    "print(\n",
    "    \"ent attention: n_layers*[batch_size, num_attention_headers, num_ents, num_toks+num_ents]\\n\",\n",
    "    ent_outputs[2][0].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(result):\n",
    "    recall = 0\n",
    "    precision_neural = [0, 0, 0, 0]\n",
    "    precision_base = [0, 0, 0, 0]\n",
    "    for target_e, cand, p_neural, p_base in result:\n",
    "        if target_e in cand:\n",
    "            recall += 1\n",
    "            if target_e == p_neural[0]:\n",
    "                precision_neural[0] += 1\n",
    "            if target_e == p_base[0]:\n",
    "                precision_base[0] += 1\n",
    "            if target_e in p_neural[:3]:\n",
    "                precision_neural[1] += 1\n",
    "            if target_e in p_neural[:5]:\n",
    "                precision_neural[2] += 1\n",
    "            if target_e in p_neural[:10]:\n",
    "                precision_neural[3] += 1\n",
    "            if target_e in p_base[:3]:\n",
    "                precision_base[1] += 1\n",
    "            if target_e in p_base[:5]:\n",
    "                precision_base[2] += 1\n",
    "            if target_e in p_base[:10]:\n",
    "                precision_base[3] += 1\n",
    "    if recall != 0:\n",
    "        return recall / len(result), [z / recall for z in precision_neural], [z / recall for z in precision_base]\n",
    "    else:\n",
    "        return 0, [0 for z in precision_neural], [0 for z in precision_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [get_precision(x[\"result\"]) for x in results]\n",
    "print(\"recall\", np.mean([x[0] for x in final_results]))\n",
    "print(\"neural\")\n",
    "print(\"p@1\", np.mean([x[1][0] for x in final_results if x[0] != 0]))\n",
    "print(\"p@3\", np.mean([x[1][1] for x in final_results if x[0] != 0]))\n",
    "print(\"p@5\", np.mean([x[1][2] for x in final_results if x[0] != 0]))\n",
    "print(\"p@10\", np.mean([x[1][3] for x in final_results if x[0] != 0]))\n",
    "print(\"base\")\n",
    "print(\"p@1\", np.mean([x[2][0] for x in final_results if x[0] != 0]))\n",
    "print(\"p@3\", np.mean([x[2][1] for x in final_results if x[0] != 0]))\n",
    "print(\"p@5\", np.mean([x[2][2] for x in final_results if x[0] != 0]))\n",
    "print(\"p@10\", np.mean([x[2][3] for x in final_results if x[0] != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"el\"></a>\n",
    "# EL\n",
    "Evaluate Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [get_precision(x[\"result\"]) for x in results]\n",
    "print(\"recall\", np.mean([x[0] for x in final_results]))\n",
    "print(\"neural\")\n",
    "print(\"p@1\", np.mean([x[1][0] for x in final_results if x[0] != 0]))\n",
    "print(\"p@3\", np.mean([x[1][1] for x in final_results if x[0] != 0]))\n",
    "print(\"p@5\", np.mean([x[1][2] for x in final_results if x[0] != 0]))\n",
    "print(\"p@10\", np.mean([x[1][3] for x in final_results if x[0] != 0]))\n",
    "print(\"base\")\n",
    "print(\"p@1\", np.mean([x[2][0] for x in final_results if x[0] != 0]))\n",
    "print(\"p@3\", np.mean([x[2][1] for x in final_results if x[0] != 0]))\n",
    "print(\"p@5\", np.mean([x[2][2] for x in final_results if x[0] != 0]))\n",
    "print(\"p@10\", np.mean([x[2][3] for x in final_results if x[0] != 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"neural\")\n",
    "print(\n",
    "    \"p@1\",\n",
    "    np.mean([x[1][0] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@3\",\n",
    "    np.mean([x[1][1] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@5\",\n",
    "    np.mean([x[1][2] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@10\",\n",
    "    np.mean([x[1][3] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\"base\")\n",
    "print(\n",
    "    \"p@1\",\n",
    "    np.mean([x[2][0] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@3\",\n",
    "    np.mean([x[2][1] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@5\",\n",
    "    np.mean([x[2][2] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@10\",\n",
    "    np.mean([x[2][3] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"neural\")\n",
    "print(\n",
    "    \"p@1\",\n",
    "    np.mean([x[1][0] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@3\",\n",
    "    np.mean([x[1][1] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@5\",\n",
    "    np.mean([x[1][2] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@10\",\n",
    "    np.mean([x[1][3] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\"base\")\n",
    "print(\n",
    "    \"p@1\",\n",
    "    np.mean([x[2][0] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@3\",\n",
    "    np.mean([x[2][1] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@5\",\n",
    "    np.mean([x[2][2] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")\n",
    "print(\n",
    "    \"p@10\",\n",
    "    np.mean([x[2][3] for i, x in enumerate(final_results) if x[0] != 0 and \"team\" not in results[i][\"headers\"][1]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dbpedia types from depedia_type_vocab.txt\n",
    "type_vocab = load_dbpedia_type_vocab(data_dir)\n",
    "config_class, model_class, _ = MODEL_CLASSES[\"EL\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.ent_type_vocab_size = len(type_vocab)\n",
    "config.mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"test_own.table_entity_linking.json\"), \"r\") as f:\n",
    "    example = json.load(f)[0]\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from [dataset].table_entity_linking.json\n",
    "test_dataset = ELDataset(\n",
    "    data_dir,\n",
    "    type_vocab,\n",
    "    max_input_tok=500,\n",
    "    src=\"test_own\",\n",
    "    max_length=[50, 10, 10, 100],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class(config, is_simple=True)\n",
    "# load the checkpoint based on mode\n",
    "checkpoint = torch.load(\n",
    "    f\"output/EL/v2/{config.mode}/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\"\n",
    ")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 10\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = ELLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size, is_train=False)\n",
    "\n",
    "# Eval!\n",
    "print(\"Num examples = %d\" % len(test_dataset))\n",
    "print(\"Batch size = %d\" % test_batch_size)\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "nb_test_steps = 0\n",
    "test_results = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    (\n",
    "        table_id,\n",
    "        input_tok,\n",
    "        input_tok_type,\n",
    "        input_tok_pos,\n",
    "        input_tok_mask,\n",
    "        input_ent_text,\n",
    "        input_ent_text_length,\n",
    "        input_ent_type,\n",
    "        input_ent_mask,\n",
    "        cand_name,\n",
    "        cand_name_length,\n",
    "        cand_description,\n",
    "        cand_description_length,\n",
    "        cand_type,\n",
    "        cand_type_length,\n",
    "        cand_mask,\n",
    "        labels,\n",
    "        entities_index,\n",
    "    ) = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    input_ent_text = input_ent_text.to(device)\n",
    "    input_ent_text_length = input_ent_text_length.to(device)\n",
    "    input_ent_type = input_ent_type.to(device)\n",
    "    input_ent_mask = input_ent_mask.to(device)\n",
    "    cand_name = cand_name.to(device)\n",
    "    cand_name_length = cand_name_length.to(device)\n",
    "    cand_description = cand_description.to(device)\n",
    "    cand_description_length = cand_description_length.to(device)\n",
    "    cand_type = cand_type.to(device)\n",
    "    cand_type_length = cand_type_length.to(device)\n",
    "    cand_mask = cand_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    if config.mode == 1:\n",
    "        cand_description = None\n",
    "        cand_description_length = None\n",
    "    elif config.mode == 2:\n",
    "        cand_type = None\n",
    "        cand_type_length = None\n",
    "    elif config.mode != 0:\n",
    "        raise Exception\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_tok_mask,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent_type,\n",
    "            input_ent_mask,\n",
    "            cand_name,\n",
    "            cand_name_length,\n",
    "            cand_description,\n",
    "            cand_description_length,\n",
    "            cand_type,\n",
    "            cand_type_length,\n",
    "            cand_mask,\n",
    "            labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        predict_index = torch.argsort(\n",
    "            prediction_scores.view(input_ent_text.size(0), input_ent_text.size(1) - 1, -1), descending=True\n",
    "        )\n",
    "        sorted_scores = (\n",
    "            torch.gather(\n",
    "                prediction_scores.view(input_ent_text.size(0), input_ent_text.size(1) - 1, -1), -1, predict_index\n",
    "            )\n",
    "        ).tolist()\n",
    "        predict_index = predict_index.tolist()\n",
    "        acc = metric.accuracy(prediction_scores, labels.view(-1), ignore_index=-1)\n",
    "        cand_length = cand_mask.sum(1).tolist()\n",
    "        ent_length = (labels != -1).sum(1).tolist()\n",
    "        for i, t_id in enumerate(table_id):\n",
    "            test_results.append(\n",
    "                [\n",
    "                    t_id,\n",
    "                    entities_index[i],\n",
    "                    [x[: cand_length[i]] for x in predict_index[i][: ent_length[i]]],\n",
    "                    [x[: cand_length[i]] for x in sorted_scores[i][: ent_length[i]]],\n",
    "                ]\n",
    "            )\n",
    "        test_loss += loss.mean().item()\n",
    "        test_acc += acc.item()\n",
    "    nb_test_steps += 1\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_acc = test_acc / nb_test_steps\n",
    "\n",
    "result = {\n",
    "    \"eval_loss\": test_loss,\n",
    "    \"eval_acc\": test_acc,\n",
    "}\n",
    "for key in sorted(result.keys()):\n",
    "    print(\"%s = %s\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dump the predictions in seperate file an use another script for official evaluation.\n",
    "# The reason is that our entity linking is based on wikidata lookup. In certain cases, the candidates\n",
    "# do not contain the target entity, such test example is still considered for metric calculation.\n",
    "# However, since there is nothing to rank we do not pass thoses examples here. So the test examples here\n",
    "# is incomplete\n",
    "with open(os.path.join(data_dir, \"test_own_entity_linking_results_0.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ct\"></a>\n",
    "# CT\n",
    "Evaluate column type annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, \"test.table_col_type.json\"), \"r\") as f:\n",
    "    example = json.load(f)[0]\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load type vocab from type_vocab.txt\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "test_dataset = WikiCTDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    type_vocab,\n",
    "    max_input_tok=500,\n",
    "    src=\"test\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2type = {idx: t for t, idx in type_vocab.items()}\n",
    "t2d_invalid = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(output, relevance_labels):\n",
    "    with torch.no_grad():\n",
    "        sorted_output = torch.argsort(output, dim=-1, descending=True)\n",
    "        sorted_labels = torch.gather(relevance_labels, -1, sorted_output).float()\n",
    "        cum_correct = torch.cumsum(sorted_labels, dim=-1)\n",
    "        cum_precision = (\n",
    "            cum_correct / torch.arange(start=1, end=cum_correct.shape[-1] + 1, device=cum_correct.device)[None, :]\n",
    "        )\n",
    "        cum_precision = cum_precision * sorted_labels\n",
    "        total_valid = torch.sum(sorted_labels, dim=-1)\n",
    "        total_valid[total_valid == 0] = 1\n",
    "        average_precision = torch.sum(cum_precision, dim=-1) / total_valid\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_type_accuracy = {}\n",
    "per_type_precision = {}\n",
    "per_type_recall = {}\n",
    "per_type_f1 = {}\n",
    "map = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}\n",
    "per_table_result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "checkpoints = [\n",
    "    \"/home/fbelotti/projects/TURL/output/logs/turl/fine-tuning-ct/2024-02-07_16-45-51/version_0/checkpoints/checkpoint-80000/pytorch_model.bin\",\n",
    "    \"/home/fbelotti/projects/TURL/output/logs/turl/fine-tuning-ct/2024-02-07_16-45-51/version_0/checkpoints/checkpoint-90000/pytorch_model.bin\",\n",
    "    \"/home/fbelotti/projects/TURL/output/logs/turl/fine-tuning-ct/2024-02-07_16-45-51/version_0/checkpoints/checkpoint-last/pytorch_model.bin\",\n",
    "    \"/home/fbelotti/projects/TURL/output/logs/turl/fine-tuning-ct/2024-02-07_18-24-49/version_0/checkpoints/checkpoint-last/model.safetensors\",\n",
    "]\n",
    "for mode in range(6):\n",
    "    if mode != 0:\n",
    "        continue\n",
    "    print(\"Mode:\", mode)\n",
    "    config_class, model_class, _ = MODEL_CLASSES[\"CT\"]\n",
    "    config = config_class.from_pretrained(config_name)\n",
    "    config.class_num = len(type_vocab)\n",
    "    config.mode = mode\n",
    "    model = model_class(config, is_simple=True)\n",
    "    checkpoint = checkpoints[-1]\n",
    "    if checkpoint.endswith(\"safetensors\"):\n",
    "        missing, unexpected = load_model(model, checkpoint, strict=True)\n",
    "        print(missing, unexpected)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    eval_batch_size = 20\n",
    "    eval_sampler = SequentialSampler(test_dataset)\n",
    "    eval_dataloader = CTLoader(test_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "    eval_loss = 0.0\n",
    "    eval_map = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    eval_targets = []\n",
    "    eval_prediction_scores = []\n",
    "    eval_pred = []\n",
    "    eval_mask = []\n",
    "    per_table_result[mode] = {}\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        (\n",
    "            table_ids,\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_tok_mask,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent,\n",
    "            input_ent_type,\n",
    "            input_ent_mask,\n",
    "            column_entity_mask,\n",
    "            column_header_mask,\n",
    "            labels_mask,\n",
    "            labels,\n",
    "        ) = batch\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        column_entity_mask = column_entity_mask.to(device)\n",
    "        column_header_mask = column_header_mask.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if mode == 1:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "        elif mode == 2:\n",
    "            input_tok_mask = input_tok_mask[:, :, : input_tok_mask.shape[1]]\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "            input_ent = None\n",
    "            input_ent_type = None\n",
    "            input_ent_mask = None\n",
    "        elif mode == 3:\n",
    "            input_ent = None\n",
    "        elif mode == 4:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent = None\n",
    "        elif mode == 5:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_tok,\n",
    "                input_tok_type,\n",
    "                input_tok_pos,\n",
    "                input_tok_mask,\n",
    "                input_ent_text,\n",
    "                input_ent_text_length,\n",
    "                input_ent,\n",
    "                input_ent_type,\n",
    "                input_ent_mask,\n",
    "                column_entity_mask,\n",
    "                column_header_mask,\n",
    "                labels_mask,\n",
    "                labels,\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "            prediction_scores = outputs[1]\n",
    "            for l_i in t2d_invalid:\n",
    "                prediction_scores[:, :, l_i] = -1000\n",
    "            for idx, table_id in enumerate(table_ids):\n",
    "                valid = labels_mask[idx].nonzero().max().item() + 1\n",
    "                if table_id not in per_table_result[mode]:\n",
    "                    per_table_result[mode][table_id] = [[], labels_mask[idx, :valid], labels[idx, :valid]]\n",
    "                per_table_result[mode][table_id][0].append(prediction_scores[idx, :valid])\n",
    "            ap = metric.average_precision(\n",
    "                prediction_scores.view(-1, config.class_num), labels.view((-1, config.class_num))\n",
    "            )\n",
    "            map = (ap * labels_mask.view(-1)).sum() / labels_mask.sum()\n",
    "            eval_loss += loss.mean().item()\n",
    "            eval_map += map.item()\n",
    "            eval_targets.extend(labels.view(-1, config.class_num).tolist())\n",
    "            eval_prediction_scores.extend(prediction_scores.view(-1, config.class_num).tolist())\n",
    "            eval_pred.extend((torch.sigmoid(prediction_scores.view(-1, config.class_num)) > 0.5).tolist())\n",
    "            eval_mask.extend(labels_mask.view(-1).tolist())\n",
    "        nb_eval_steps += 1\n",
    "    print(eval_map / nb_eval_steps)\n",
    "    eval_targets = np.array(eval_targets)\n",
    "    eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "    eval_mask = np.array(eval_mask)\n",
    "    eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "    eval_pred = np.array(eval_pred)\n",
    "    eval_tp = eval_mask[:, np.newaxis] * eval_pred * eval_targets\n",
    "    eval_precision = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_pred, axis=0)\n",
    "    eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "    eval_recall = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "    eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "    eval_f1 = 2 * eval_precision * eval_recall / (eval_precision + eval_recall)\n",
    "    eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "    per_type_instance_num = np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "    per_type_instance_num[per_type_instance_num == 0] = 1\n",
    "    per_type_correct_instance_num = np.sum(\n",
    "        eval_mask[:, np.newaxis] * (eval_prediction_ranks < eval_targets.sum(axis=1)[:, np.newaxis]) * eval_targets,\n",
    "        axis=0,\n",
    "    )\n",
    "    per_type_accuracy[mode] = per_type_correct_instance_num / per_type_instance_num\n",
    "    per_type_precision[mode] = eval_precision\n",
    "    per_type_recall[mode] = eval_recall\n",
    "    per_type_f1[mode] = eval_f1\n",
    "    precision[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_pred)\n",
    "    recall[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_targets)\n",
    "    f1[mode] = 2 * precision[mode] * recall[mode] / (precision[mode] + recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr = 0\n",
    "total_valid = 0\n",
    "errors = []\n",
    "for table_id, result in per_table_result[3].items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    current_corr = 0\n",
    "    for col_idx, pred in enumerate(prediction_scores.argmax(-1).tolist()):\n",
    "        current_corr += label[col_idx, pred].item()\n",
    "    total_valid += label_mask.sum().item()\n",
    "    total_corr += current_corr\n",
    "    if current_corr != label_mask.sum().item():\n",
    "        errors.append(table_id)\n",
    "print(total_corr / total_valid, total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, i in sorted(type_vocab.items(), key=lambda z: -per_type_instance_num[z[1]]):\n",
    "    print(\n",
    "        \"%s %.4f %.4f %.4f %.4f %.4f  %.4f %.4f\"\n",
    "        % (\n",
    "            t,\n",
    "            per_type_instance_num[i],\n",
    "            per_type_f1[0][i],\n",
    "            per_type_f1[4][i],\n",
    "            per_type_f1[1][i],\n",
    "            per_type_f1[3][i],\n",
    "            per_type_f1[2][i],\n",
    "            per_type_f1[5][i],\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type mapping is used to map the types used in some other datasets to our types, so we can directly evaluate without retraining our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    \"Election\": [\"government.election\"],\n",
    "    \"Film\": [\"film.film\"],\n",
    "    \"mountain\": [\"geography.mountain\"],\n",
    "    \"Building\": [\"architecture.building\"],\n",
    "    \"RadioStation\": [\"broadcast.radio_station\"],\n",
    "    \"TelevisionShow\": [\"tv.tv_program\"],\n",
    "    \"Country\": [\"location.country\"],\n",
    "    \"Airport\": [\"aviation.airport\"],\n",
    "    \"AdministrativeRegion\": [\"location.region\"],\n",
    "    \"University\": [\"education.university\"],\n",
    "    \"Newspaper\": [\"book.newspaper\"],\n",
    "    \"FictionalCharacter\": [\"fictional_universe.fictional_character\"],\n",
    "    \"Currency\": [\"finance.currency\"],\n",
    "    \"Novel\": [\"book.book\"],\n",
    "    \"Wrestler\": [\"sports.pro_athlete\"],\n",
    "    \"swimmer\": [\"sports.pro_athlete\"],\n",
    "    \"GolfPlayer\": [\"sports.golfer\", \"sports.pro_athlete\"],\n",
    "    \"Book\": [\"book.book\"],\n",
    "    \"Political Party\": [\"government.political_party\"],\n",
    "    \"Person\": [\"people.person\"],\n",
    "    \"VideoGame\": [\"cvg.computer_videogame\"],\n",
    "    \"Animal\": [\"biology.animal\"],\n",
    "    \"PoliticalParty\": [\"government.political_party\"],\n",
    "    \"BaseballPlayer\": [\"sports.pro_athlete\"],\n",
    "    \"Monarch\": [\"royalty.monarch\"],\n",
    "    \"Mountain\": [\"geography.mountain\"],\n",
    "    \"City\": [\"location.citytown\"],\n",
    "    \"Company\": [\"business.consumer_company\"],\n",
    "    \"cricketer\": [\"sports.pro_athlete\"],\n",
    "    \"Airline\": [\"aviation.airline\"],\n",
    "}\n",
    "t2d_types = set([y for _, x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t, i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    \"City\": [\"location.citytown\"],\n",
    "    \"VideoGame\": [\"cvg.computer_videogame\"],\n",
    "    \"Mountain\": [\"geography.mountain\"],\n",
    "    \"Museum\": [],\n",
    "    \"Writer\": [\"film.writer\", \"tv.tv_writer\", \"music.writer\", \"book.author\"],\n",
    "    \"Lake\": [],\n",
    "    \"AdministrativeRegion\": [\"location.administrative_division\"],\n",
    "    \"Book\": [\"book.book\"],\n",
    "    \"Saint\": [],\n",
    "    \"Monarch\": [\"royalty.monarch\"],\n",
    "    \"Bird\": [],\n",
    "    \"Plant\": [],\n",
    "    \"Mayor\": [],\n",
    "    \"Currency\": [\"finance.currency\"],\n",
    "    \"MovieDirector\": [\"film.director\"],\n",
    "    \"Company\": [\n",
    "        \"film.production_company\",\n",
    "        \"automotive.company\",\n",
    "        \"business.consumer_company\",\n",
    "        \"business.defunct_company\",\n",
    "    ],\n",
    "    \"Genre\": [\n",
    "        \"cvg.cvg_genre\",\n",
    "        \"film.film_genre\",\n",
    "        \"broadcast.genre\",\n",
    "        \"media_common.media_genre\",\n",
    "        \"tv.tv_genre\",\n",
    "        \"music.genre\",\n",
    "    ],\n",
    "    \"GovernmentType\": [\"government.governmental_body\"],\n",
    "    \"Hospital\": [],\n",
    "    \"Building\": [\"architecture.building\"],\n",
    "    \"PoliticalParty\": [\"government.political_party\"],\n",
    "    \"Language\": [\"language.human_language\"],\n",
    "    \"Country\": [\"location.country\"],\n",
    "    \"University\": [\"education.university\"],\n",
    "    \"SportsTeam\": [\"sports.sports_team\"],\n",
    "    \"RadioStation\": [\"broadcast.radio_station\"],\n",
    "    \"Airport\": [\"aviation.airport\"],\n",
    "    \"Airline\": [\"aviation.airline\"],\n",
    "    \"Wrestler\": [],\n",
    "    \"Newspaper\": [\"book.newspaper\"],\n",
    "    \"Mammal\": [],\n",
    "    \"MountainRange\": [],\n",
    "    \"BaseballPlayer\": [\"baseball.baseball_player\"],\n",
    "    \"AcademicJournal\": [],\n",
    "    \"Scientist\": [],\n",
    "    \"Continent\": [],\n",
    "    \"Film\": [\"film.film\"],\n",
    "}\n",
    "\n",
    "t2d_types = set([y for _, x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t, i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_type_mapping = {\n",
    "    \"Film\": [\"film.film\"],\n",
    "    \"Lake\": [],\n",
    "    \"Language\": [\"language.human_language\"],\n",
    "    \"Country\": [\"location.country\"],\n",
    "    \"Company\": [\n",
    "        \"film.production_company\",\n",
    "        \"automotive.company\",\n",
    "        \"business.consumer_company\",\n",
    "        \"business.defunct_company\",\n",
    "    ],\n",
    "    \"Person\": [\"people.person\"],\n",
    "    \"VideoGame\": [\"cvg.computer_videogame\"],\n",
    "    \"City\": [\"location.citytown\"],\n",
    "    \"Currency\": [\"finance.currency\"],\n",
    "    \"Bird\": [],\n",
    "    \"Mountain\": [\"geography.mountain\"],\n",
    "    \"Scientist\": [],\n",
    "    \"Plant\": [],\n",
    "    \"TelevisionShow\": [\"tv.tv_program\"],\n",
    "    \"Animal\": [],\n",
    "    \"AdministrativeRegion\": [\"location.administrative_division\"],\n",
    "    \"Genre\": [\n",
    "        \"cvg.cvg_genre\",\n",
    "        \"film.film_genre\",\n",
    "        \"broadcast.genre\",\n",
    "        \"media_common.media_genre\",\n",
    "        \"tv.tv_genre\",\n",
    "        \"music.genre\",\n",
    "    ],\n",
    "    \"Newspaper\": [\"book.newspaper\"],\n",
    "    \"Airport\": [\"aviation.airport\"],\n",
    "    \"AcademicJournal\": [],\n",
    "    \"PopulatedPlace\": [],\n",
    "    \"Wrestler\": [],\n",
    "    \"PoliticalParty\": [\"government.political_party\"],\n",
    "    \"Cricketer\": [\"cricket.cricket_player\"],\n",
    "    \"Eukaryote\": [],\n",
    "    \"Saint\": [],\n",
    "    \"Writer\": [\"film.writer\", \"tv.tv_writer\", \"music.writer\", \"book.author\"],\n",
    "    \"Museum\": [],\n",
    "    \"BaseballPlayer\": [\"baseball.baseball_player\"],\n",
    "    \"EducationalInstitution\": [\"education.educational_institution\"],\n",
    "    \"GovernmentType\": [\"government.governmental_body\"],\n",
    "    \"SportsTeam\": [\"sports.sports_team\"],\n",
    "}\n",
    "\n",
    "reverse_type_mapping = {t2: t1 for t1, t2s in t2d_type_mapping.items() for t2 in t2s}\n",
    "\n",
    "t2d_types = set([y for _, x in t2d_type_mapping.items() for y in x])\n",
    "t2d_invalid = []\n",
    "for t, i in type_vocab.items():\n",
    "    if t not in t2d_types:\n",
    "        t2d_invalid.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "pred = 0\n",
    "tp = 0\n",
    "for table_id, result in per_table_result[4].items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    current_corr = 0\n",
    "    for col_idx in range(label.shape[0]):\n",
    "        if label_mask[col_idx] != 0:\n",
    "            gt_t = set([reverse_type_mapping[id2type[t]] for t in label[col_idx].nonzero()[0].tolist()])\n",
    "            if (prediction_scores[col_idx] > 0).nonzero().shape[0] > 0:\n",
    "                pred_t = set(\n",
    "                    [reverse_type_mapping[id2type[t]] for t in (prediction_scores[col_idx] > 0).nonzero()[0].tolist()]\n",
    "                )\n",
    "            else:\n",
    "                pred_t = set()\n",
    "            p += len(gt_t)\n",
    "            pred += len(pred_t)\n",
    "            tp += len(gt_t & pred_t)\n",
    "precision = tp / pred\n",
    "recall = tp / p\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[1].nonzero()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 if label_mask[1] == 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_table_result[\"64499281_8_7181683886563136802\"][0][1].argsort(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT - Semtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/Semtab\"\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "test_dataset = WikiCTDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    type_vocab,\n",
    "    max_input_tok=500,\n",
    "    src=\"wiki_test30\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(type_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2type = {y: x for x, y in type_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(output, relevance_labels):\n",
    "    with torch.no_grad():\n",
    "        sorted_output = torch.argsort(output, dim=-1, descending=True)\n",
    "        sorted_labels = torch.gather(relevance_labels, -1, sorted_output).float()\n",
    "        cum_correct = torch.cumsum(sorted_labels, dim=-1)\n",
    "        cum_precision = (\n",
    "            cum_correct / torch.arange(start=1, end=cum_correct.shape[-1] + 1, device=cum_correct.device)[None, :]\n",
    "        )\n",
    "        cum_precision = cum_precision * sorted_labels\n",
    "        total_valid = torch.sum(sorted_labels, dim=-1)\n",
    "        total_valid[total_valid == 0] = 1\n",
    "        average_precision = torch.sum(cum_precision, dim=-1) / total_valid\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_type_accuracy = {}\n",
    "per_type_precision = {}\n",
    "per_type_recall = {}\n",
    "per_type_f1 = {}\n",
    "map = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "checkpoint = \"output/CT/Semtab/wiki_train70/4/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\"\n",
    "mode = 5\n",
    "print(\"Mode:\", mode)\n",
    "config_class, model_class, _ = MODEL_CLASSES[\"CT\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.class_num = len(type_vocab)\n",
    "config.mode = mode\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_batch_size = 20\n",
    "eval_sampler = SequentialSampler(test_dataset)\n",
    "eval_dataloader = CTLoader(test_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "eval_loss = 0.0\n",
    "eval_map = 0.0\n",
    "nb_eval_steps = 0\n",
    "eval_targets = []\n",
    "eval_prediction_scores = []\n",
    "eval_pred = []\n",
    "eval_mask = []\n",
    "per_table_result = {}\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    (\n",
    "        table_ids,\n",
    "        input_tok,\n",
    "        input_tok_type,\n",
    "        input_tok_pos,\n",
    "        input_tok_mask,\n",
    "        input_ent_text,\n",
    "        input_ent_text_length,\n",
    "        input_ent,\n",
    "        input_ent_type,\n",
    "        input_ent_mask,\n",
    "        column_entity_mask,\n",
    "        column_header_mask,\n",
    "        labels_mask,\n",
    "        labels,\n",
    "    ) = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    input_ent_text = input_ent_text.to(device)\n",
    "    input_ent_text_length = input_ent_text_length.to(device)\n",
    "    input_ent = input_ent.to(device)\n",
    "    input_ent_type = input_ent_type.to(device)\n",
    "    input_ent_mask = input_ent_mask.to(device)\n",
    "    column_entity_mask = column_entity_mask.to(device)\n",
    "    column_header_mask = column_header_mask.to(device)\n",
    "    labels_mask = labels_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    if mode == 1:\n",
    "        input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "    elif mode == 2:\n",
    "        input_tok_mask = input_tok_mask[:, :, : input_tok_mask.shape[1]]\n",
    "        input_ent_text = None\n",
    "        input_ent_text_length = None\n",
    "        input_ent = None\n",
    "        input_ent_type = None\n",
    "        input_ent_mask = None\n",
    "    elif mode == 3:\n",
    "        input_ent = None\n",
    "    elif mode == 4:\n",
    "        input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "        input_ent = None\n",
    "    elif mode == 5:\n",
    "        input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "        input_tok = None\n",
    "        input_tok_type = None\n",
    "        input_tok_pos = None\n",
    "        input_tok_mask = None\n",
    "        input_ent_text = None\n",
    "        input_ent_text_length = None\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_tok_mask,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent,\n",
    "            input_ent_type,\n",
    "            input_ent_mask,\n",
    "            column_entity_mask,\n",
    "            column_header_mask,\n",
    "            labels_mask,\n",
    "            labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        for idx, table_id in enumerate(table_ids):\n",
    "            valid = labels_mask[idx].nonzero().max().item() + 1\n",
    "            if table_id not in per_table_result:\n",
    "                per_table_result[table_id] = [[], labels_mask[idx, :valid], labels[idx, :valid]]\n",
    "            per_table_result[table_id][0].append(prediction_scores[idx, :valid])\n",
    "\n",
    "        eval_loss += loss.mean().item()\n",
    "        eval_targets.extend(labels.view(-1, config.class_num).tolist())\n",
    "        eval_prediction_scores.extend(prediction_scores.view(-1, config.class_num).tolist())\n",
    "        eval_pred.extend(\n",
    "            (\n",
    "                prediction_scores.view(-1, config.class_num)\n",
    "                == prediction_scores.view(-1, config.class_num).max(-1)[0][:, None]\n",
    "            ).tolist()\n",
    "        )\n",
    "        eval_mask.extend(labels_mask.view(-1).tolist())\n",
    "    nb_eval_steps += 1\n",
    "eval_targets = np.array(eval_targets)\n",
    "eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "eval_mask = np.array(eval_mask)\n",
    "eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "eval_pred = np.array(eval_pred)\n",
    "eval_tp = eval_mask[:, np.newaxis] * eval_pred * eval_targets\n",
    "eval_precision = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_pred, axis=0)\n",
    "eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "eval_recall = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "eval_f1 = 2 * eval_precision * eval_recall / (eval_precision + eval_recall)\n",
    "eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "per_type_instance_num = np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "per_type_instance_num[per_type_instance_num == 0] = 1\n",
    "per_type_correct_instance_num = np.sum(\n",
    "    eval_mask[:, np.newaxis] * (eval_prediction_ranks < eval_targets.sum(axis=1)[:, np.newaxis]) * eval_targets, axis=0\n",
    ")\n",
    "per_type_accuracy[mode] = per_type_correct_instance_num / per_type_instance_num\n",
    "per_type_precision[mode] = eval_precision\n",
    "per_type_recall[mode] = eval_recall\n",
    "per_type_f1[mode] = eval_f1\n",
    "precision[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_pred)\n",
    "recall[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_targets)\n",
    "f1[mode] = 2 * precision[mode] * recall[mode] / (precision[mode] + recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_types = [\n",
    "    \"City\",\n",
    "    \"VideoGame\",\n",
    "    \"Mountain\",\n",
    "    \"Writer\",\n",
    "    \"Lake\",\n",
    "    \"AdministrativeRegion\",\n",
    "    \"Book\",\n",
    "    \"Saint\",\n",
    "    \"Monarch\",\n",
    "    \"Bird\",\n",
    "    \"Plant\",\n",
    "    \"Currency\",\n",
    "    \"Company\",\n",
    "    \"Genre\",\n",
    "    \"Building\",\n",
    "    \"PoliticalParty\",\n",
    "    \"Language\",\n",
    "    \"Country\",\n",
    "    \"University\",\n",
    "    \"SportsTeam\",\n",
    "    \"RadioStation\",\n",
    "    \"Airport\",\n",
    "    \"Wrestler\",\n",
    "    \"Newspaper\",\n",
    "    \"Mammal\",\n",
    "    \"Mayor\",\n",
    "    \"AcademicJournal\",\n",
    "    \"Scientist\",\n",
    "    \"Continent\",\n",
    "    \"Film\",\n",
    "    \"BaseballPlayer\",\n",
    "]\n",
    "non_wiki_types = [x for x in type_vocab if x not in wiki_types]\n",
    "wiki_types = set([type_vocab[x] for x in wiki_types])\n",
    "wiki_type_mask = torch.full((len(type_vocab),), -10000.0).to(device)\n",
    "for i in wiki_types:\n",
    "    wiki_type_mask[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, i in type_vocab.items():\n",
    "    print(t, per_type_f1[4][i], per_type_instance_num[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_wiki_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr = 0\n",
    "total_valid = 0\n",
    "errors = []\n",
    "for table_id, result in per_table_result.items():\n",
    "    prediction_scores, label_mask, label = result\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    prediction_scores[:, 15] = torch.where(\n",
    "        prediction_scores[:, 15] > prediction_scores[:, 27], prediction_scores[:, 15], prediction_scores[:, 27]\n",
    "    )\n",
    "    prediction_scores += wiki_type_mask[None, :]\n",
    "    pred_acc = ((prediction_scores == prediction_scores.max(-1)[0][:, None]) * label).sum(-1)\n",
    "    total_valid += label_mask.sum().item()\n",
    "    total_corr += pred_acc.sum().item()\n",
    "    if pred_acc.sum().item() != label_mask.sum().item():\n",
    "        errors.append(table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inspect_id in errors:\n",
    "    print(inspect_id)\n",
    "    prediction_scores, label_mask, label = per_table_result[inspect_id]\n",
    "    prediction_scores = torch.stack(prediction_scores, 0).mean(0)\n",
    "    for col_id in range(label.shape[0]):\n",
    "        if label[col_id].sum().item() == 0:\n",
    "            continue\n",
    "        display(id2type[label[col_id].nonzero().item()])\n",
    "        display(\n",
    "            [\n",
    "                [id2type[l], prediction_scores[col_id, l].item()]\n",
    "                for l in prediction_scores[col_id].argsort().tolist()[::-1][:3]\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_table_result[\"Baseball_Hall_of_Fame_balloting,_2015#0\"][0][0].argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"re\"></a>\n",
    "# RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab = load_relation_vocab(data_dir)\n",
    "config.class_num = len(type_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = REDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    type_vocab,\n",
    "    max_input_tok=500,\n",
    "    src=\"test\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_type_accuracy = {}\n",
    "per_type_precision = {}\n",
    "per_type_recall = {}\n",
    "per_type_f1 = {}\n",
    "map = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "config_name = \"configs/table-base-config_v2.json\"\n",
    "checkpoints = [\n",
    "    \"output/RE/v2/0/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "    \"output/RE/v2/1/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "    \"output/RE/v2/2/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "    \"output/RE/v2/3/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "    \"output/RE/v2/4/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "    \"output/RE/v2/5/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam/pytorch_model.bin\",\n",
    "]\n",
    "for mode in [0, 1, 2, 3, 4, 5]:\n",
    "    print(mode)\n",
    "    config_class, model_class, _ = MODEL_CLASSES[\"RE\"]\n",
    "    config = config_class.from_pretrained(config_name)\n",
    "    config.class_num = len(type_vocab)\n",
    "    config.mode = mode\n",
    "    model = model_class(config, is_simple=True)\n",
    "    checkpoint = checkpoints[mode]\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    eval_batch_size = 20\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = RELoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "    eval_loss = 0.0\n",
    "    eval_map = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    eval_targets = []\n",
    "    eval_prediction_scores = []\n",
    "    eval_pred = []\n",
    "    eval_mask = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        (\n",
    "            table_id,\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_tok_mask,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent,\n",
    "            input_ent_type,\n",
    "            input_ent_mask,\n",
    "            column_entity_mask,\n",
    "            column_header_mask,\n",
    "            labels_mask,\n",
    "            labels,\n",
    "        ) = batch\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        column_entity_mask = column_entity_mask.to(device)\n",
    "        column_header_mask = column_header_mask.to(device)\n",
    "        labels_mask = labels_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        if mode == 1:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "        elif mode == 2:\n",
    "            input_tok_mask = input_tok_mask[:, :, : input_tok_mask.shape[1]]\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "            input_ent = None\n",
    "            input_ent_type = None\n",
    "            input_ent_mask = None\n",
    "        elif mode == 3:\n",
    "            input_ent = None\n",
    "        elif mode == 4:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent = None\n",
    "        elif mode == 5:\n",
    "            input_ent_mask = input_ent_mask[:, :, input_tok_mask.shape[1] :]\n",
    "            input_tok = None\n",
    "            input_tok_type = None\n",
    "            input_tok_pos = None\n",
    "            input_tok_mask = None\n",
    "            input_ent_text = None\n",
    "            input_ent_text_length = None\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_tok,\n",
    "                input_tok_type,\n",
    "                input_tok_pos,\n",
    "                input_tok_mask,\n",
    "                input_ent_text,\n",
    "                input_ent_text_length,\n",
    "                input_ent,\n",
    "                input_ent_type,\n",
    "                input_ent_mask,\n",
    "                column_entity_mask,\n",
    "                column_header_mask,\n",
    "                labels_mask,\n",
    "                labels,\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "            prediction_scores = outputs[1]\n",
    "            # pdb.set_trace()\n",
    "            ap = metric.average_precision(\n",
    "                prediction_scores.view(-1, config.class_num), labels.view((-1, config.class_num))\n",
    "            )\n",
    "            map = (ap * labels_mask.view(-1)).sum() / labels_mask.sum()\n",
    "            eval_loss += loss.mean().item()\n",
    "            eval_map += map.item()\n",
    "            eval_targets.extend(labels.view(-1, config.class_num).tolist())\n",
    "            eval_prediction_scores.extend(prediction_scores.view(-1, config.class_num).tolist())\n",
    "            eval_pred.extend((torch.sigmoid(prediction_scores.view(-1, config.class_num)) > 0.5).tolist())\n",
    "            eval_mask.extend(labels_mask.view(-1).tolist())\n",
    "        nb_eval_steps += 1\n",
    "    print(eval_map / nb_eval_steps)\n",
    "    eval_targets = np.array(eval_targets)\n",
    "    eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "    eval_mask = np.array(eval_mask)\n",
    "    eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "    eval_pred = np.array(eval_pred)\n",
    "    eval_tp = eval_mask[:, np.newaxis] * eval_pred * eval_targets\n",
    "    eval_precision = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_pred, axis=0)\n",
    "    eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "    eval_recall = np.sum(eval_tp, axis=0) / np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "    eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "    eval_f1 = 2 * eval_precision * eval_recall / (eval_precision + eval_recall)\n",
    "    eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "    per_type_instance_num = np.sum(eval_mask[:, np.newaxis] * eval_targets, axis=0)\n",
    "    per_type_instance_num[per_type_instance_num == 0] = 1\n",
    "    per_type_correct_instance_num = np.sum(\n",
    "        eval_mask[:, np.newaxis] * (eval_prediction_ranks < eval_targets.sum(axis=1)[:, np.newaxis]) * eval_targets,\n",
    "        axis=0,\n",
    "    )\n",
    "    per_type_accuracy[mode] = per_type_correct_instance_num / per_type_instance_num\n",
    "    per_type_precision[mode] = eval_precision\n",
    "    per_type_recall[mode] = eval_recall\n",
    "    per_type_f1[mode] = eval_f1\n",
    "    precision[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_pred)\n",
    "    recall[mode] = np.sum(eval_tp) / np.sum(eval_mask[:, np.newaxis] * eval_targets)\n",
    "    f1[mode] = 2 * precision[mode] * recall[mode] / (precision[mode] + recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, i in sorted(type_vocab.items(), key=lambda z: -per_type_instance_num[z[1]]):\n",
    "    print(\n",
    "        \"%s %.4f %.4f %.4f %.4f %.4f  %.4f\"\n",
    "        % (\n",
    "            t,\n",
    "            per_type_instance_num[i],\n",
    "            per_type_f1[5][i],\n",
    "            per_type_f1[0][i],\n",
    "            per_type_f1[1][i],\n",
    "            per_type_f1[3][i],\n",
    "            per_type_f1[2][i],\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"%s %.4f %.4f %.4f %.4f %.4f  %.4f\"\n",
    "        % (\n",
    "            t,\n",
    "            per_type_instance_num[i],\n",
    "            per_type_precision[5][i],\n",
    "            per_type_precision[0][i],\n",
    "            per_type_precision[1][i],\n",
    "            per_type_precision[3][i],\n",
    "            per_type_precision[2][i],\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"%s %.4f %.4f %.4f %.4f %.4f  %.4f\"\n",
    "        % (\n",
    "            t,\n",
    "            per_type_instance_num[i],\n",
    "            per_type_recall[5][i],\n",
    "            per_type_recall[0][i],\n",
    "            per_type_recall[1][i],\n",
    "            per_type_recall[3][i],\n",
    "            per_type_recall[2][i],\n",
    "        )\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = REBERTDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    type_vocab,\n",
    "    max_input_tok=500,\n",
    "    src=\"dev\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "mode = -1\n",
    "config_name = \"configs/tiny-bert-config.json\"\n",
    "checkpoint = \"output/RE_Bert/v2/pytorch_model.bin\"\n",
    "config_class, model_class, _ = MODEL_CLASSES[\"REBERT\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.num_labels = len(type_vocab)\n",
    "config.mode = mode\n",
    "model = model_class(config)\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_batch_size = 20\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = REBERTLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False)\n",
    "eval_loss = 0.0\n",
    "eval_map = 0.0\n",
    "nb_eval_steps = 0\n",
    "eval_targets = []\n",
    "eval_prediction_scores = []\n",
    "eval_pred = []\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    table_id, input_tok, input_tok_mask, labels = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tok, attention_mask=input_tok_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        # pdb.set_trace()\n",
    "        ap = metric.average_precision(\n",
    "            prediction_scores.view(-1, config.num_labels), labels.view((-1, config.num_labels))\n",
    "        )\n",
    "        map = map = ap.sum() / len(ap)\n",
    "        eval_loss += loss.mean().item()\n",
    "        eval_map += map.item()\n",
    "        eval_targets.extend(labels.view(-1, config.num_labels).tolist())\n",
    "        eval_prediction_scores.extend(prediction_scores.view(-1, config.num_labels).tolist())\n",
    "        eval_pred.extend((torch.sigmoid(prediction_scores.view(-1, config.num_labels)) > 0.5).tolist())\n",
    "    nb_eval_steps += 1\n",
    "print(eval_map / nb_eval_steps)\n",
    "eval_targets = np.array(eval_targets)\n",
    "eval_prediction_scores = np.array(eval_prediction_scores)\n",
    "eval_prediction_ranks = np.argsort(np.argsort(-eval_prediction_scores))\n",
    "eval_pred = np.array(eval_pred)\n",
    "eval_tp = eval_pred * eval_targets\n",
    "eval_precision = np.sum(eval_tp, axis=0) / np.sum(eval_pred, axis=0)\n",
    "eval_precision = np.nan_to_num(eval_precision, 1)\n",
    "eval_recall = np.sum(eval_tp, axis=0) / np.sum(eval_targets, axis=0)\n",
    "eval_recall = np.nan_to_num(eval_recall, 1)\n",
    "eval_f1 = 2 * eval_precision * eval_recall / (eval_precision + eval_recall)\n",
    "eval_f1 = np.nan_to_num(eval_f1, 0)\n",
    "per_type_instance_num = np.sum(eval_targets, axis=0)\n",
    "per_type_instance_num[per_type_instance_num == 0] = 1\n",
    "per_type_correct_instance_num = np.sum(\n",
    "    (eval_prediction_ranks < eval_targets.sum(axis=1)[:, np.newaxis]) * eval_targets, axis=0\n",
    ")\n",
    "per_type_accuracy[mode] = per_type_correct_instance_num / per_type_instance_num\n",
    "per_type_precision[mode] = eval_precision\n",
    "per_type_recall[mode] = eval_recall\n",
    "per_type_f1[mode] = eval_f1\n",
    "precision[mode] = np.sum(eval_tp) / np.sum(eval_pred)\n",
    "recall[mode] = np.sum(eval_tp) / np.sum(eval_targets)\n",
    "f1[mode] = 2 * precision[mode] * recall[mode] / (precision[mode] + recall[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    print(k)\n",
    "    maps_base = []\n",
    "    recalls = []\n",
    "    for i, x in enumerate(tqdm(test_dataset)):\n",
    "        header_count = {i: 0.0 for i in range(config.header_vocab_size)}\n",
    "        dist, neighbor = neigh.kneighbors(tfidf.transform([x[1][1 : x[5] + 1]]), k, return_distance=True)\n",
    "        dist = dist.reshape(-1)\n",
    "        for j, n in enumerate(neighbor.reshape([-1])):\n",
    "            for h in train_dataset[n][6]:\n",
    "                if dist[j] == 0:\n",
    "                    header_count[h] += 100\n",
    "                else:\n",
    "                    header_count[h] += 1 / dist[j]\n",
    "        target_e = set(x[6])\n",
    "        recalls.append(len([z for z in header_count if z in target_e]) / len(target_e))\n",
    "        ap = average_precision(\n",
    "            [1 if z in target_e else 0 for z, _ in sorted(header_count.items(), key=lambda p: p[1], reverse=True)]\n",
    "        )\n",
    "        maps_base.append(ap)\n",
    "    print(np.mean(maps_base))\n",
    "    print(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(maps_base))\n",
    "print(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    print(k)\n",
    "    maps_base = []\n",
    "    recalls = []\n",
    "    for i, x in enumerate(tqdm(test_dataset)):\n",
    "        header_count = {i: 0.0 for i in range(config.header_vocab_size)}\n",
    "        dist, neighbor = neigh.kneighbors(tfidf.transform([x[1][1 : x[5] + 1]]), k, return_distance=True)\n",
    "        dist = dist.reshape(-1)\n",
    "        target_e = set(x[6][1:])\n",
    "        seed = x[6][0]\n",
    "        for j, n in enumerate(neighbor.reshape([-1])):\n",
    "            label_score = 1 if seed in train_dataset[n][6] else 0.00001\n",
    "            for h in train_dataset[n][6]:\n",
    "                if h != seed:\n",
    "                    if dist[j] == 0:\n",
    "                        header_count[h] += label_score * 100\n",
    "                    else:\n",
    "                        header_count[h] += label_score * 1 / dist[j]\n",
    "        recalls.append(len([z for z in header_count if z in target_e]) / len(target_e))\n",
    "        ap = average_precision(\n",
    "            [1 if z in target_e else 0 for z, _ in sorted(header_count.items(), key=lambda p: p[1], reverse=True)]\n",
    "        )\n",
    "        maps_base.append(ap)\n",
    "    print(np.mean(maps_base))\n",
    "    print(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for k in [10]:\n",
    "    maps_base = []\n",
    "    recalls = []\n",
    "    for i, x in enumerate(tqdm(eval_dataset)):\n",
    "        header_count = {i: 0.0 for i in range(config.header_vocab_size)}\n",
    "        dist, neighbor = neigh.kneighbors(tfidf.transform([x[1][1 : x[5] + 1]]), k, return_distance=True)\n",
    "        dist = dist.reshape(-1)\n",
    "        target_e = set(x[6][1:])\n",
    "        seed = x[6][0]\n",
    "        neighbor = neighbor.reshape([-1])\n",
    "        for j, n in enumerate(neighbor):\n",
    "            label_score = 1 if seed in train_dataset[n][6] else 0.00001\n",
    "            for h in train_dataset[n][6]:\n",
    "                if h != seed:\n",
    "                    if dist[j] == 0:\n",
    "                        header_count[h] += label_score * 100\n",
    "                    else:\n",
    "                        header_count[h] += label_score * 1 / dist[j]\n",
    "        recalls.append(len([z for z in header_count if z in target_e]) / len(target_e))\n",
    "        sorted_base = sorted(header_count.items(), key=lambda p: p[1], reverse=True)\n",
    "        ap = average_precision([1 if z in target_e else 0 for z, _ in sorted_base])\n",
    "        maps_base.append(ap)\n",
    "        if ap > maps[i]:\n",
    "            print(\"base: {},ours: {}\".format(ap, maps[i]))\n",
    "            print(x[0])\n",
    "            print(\"caption\", train_dataset.tokenizer.decode(x[1][1 : x[5] + 1]))\n",
    "            display([train_dataset.header_vocab[z] for z in eval_dataset[i][6]])\n",
    "            ranked_our = np.argsort(results[i])[::-1]\n",
    "            display([train_dataset.header_vocab[z] for z in ranked_our[:10]])\n",
    "            print(\"neighbor\")\n",
    "            print(train_dataset[neighbor[0]][0])\n",
    "            print(\n",
    "                \"caption\",\n",
    "                train_dataset.tokenizer.decode(train_dataset[neighbor[0]][1][1 : train_dataset[neighbor[0]][5] + 1]),\n",
    "            )\n",
    "            display([train_dataset.header_vocab[z] for z, _ in sorted_base[:10]])\n",
    "    print(np.mean(maps_base))\n",
    "    print(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"output/hybrid/model_v1_table_0.2_0.6_0.7_30000_1e-4_with_cand_0\"\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(os.path.join(checkpoint, \"pytorch_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint[\"table.embeddings.ent_embeddings.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_loc = \"output/hybrid/model_v1_table_0.2_0.6_0.7_30000_1e-4_with_cand_0\"\n",
    "entity_vocab_with_type = []\n",
    "with open(\"data/wikisql_entity/entity_vocab_with_type.tsv\", \"r\", encoding=\"utf8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        wiki_id = line.strip().split(\"\\t\")[0]\n",
    "        entity_vocab_with_type.append(int(wiki_id))\n",
    "with open(os.path.join(dump_loc, \"entity_embedding_with_type.tsv\"), \"w\") as f_e:\n",
    "    for wiki_id in entity_vocab_with_type:\n",
    "        f_e.write(\n",
    "            \"{}\\n\".format(\n",
    "                \"\\t\".join(\n",
    "                    [\n",
    "                        str(z)\n",
    "                        for z in checkpoint[\"table.embeddings.ent_embeddings.weight\"][entity_wikid2id[wiki_id]].tolist()\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiHybridTableDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    max_cell=100,\n",
    "    max_input_tok=350,\n",
    "    max_input_ent=150,\n",
    "    src=\"train\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    "    mode=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x[8] - 2 for x in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WikiHybridTableDataset(\n",
    "    data_dir,\n",
    "    entity_vocab,\n",
    "    max_cell=100,\n",
    "    max_input_tok=350,\n",
    "    max_input_ent=150,\n",
    "    src=\"dev\",\n",
    "    max_length=[50, 10, 10],\n",
    "    force_new=False,\n",
    "    tokenizer=None,\n",
    "    mode=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./output/CER/v2/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam_seed_1_10000/pytorch_model.bin\"\n",
    "\n",
    "\n",
    "config_class, model_class, _ = MODEL_CLASSES[\"CER\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True\n",
    "\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_entity_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entity_set = set(dataset.entity_wikid2id.keys())\n",
    "tables_ignored = 0\n",
    "cached_baseline = \"data/wikitables_v2/CER_test_result.pkl\"\n",
    "with open(cached_baseline, \"rb\") as f:\n",
    "    cached_baseline_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 1\n",
    "results = {}\n",
    "with open(os.path.join(data_dir, \"test_tables.jsonl\"), \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        table = json.loads(line.strip())\n",
    "        table_id = table.get(\"_id\", \"\")\n",
    "        pgEnt = table[\"pgId\"]\n",
    "        if not pgEnt in all_entity_set:\n",
    "            pgEnt = -1\n",
    "        pgTitle = table.get(\"pgTitle\", \"\").lower()\n",
    "        secTitle = table.get(\"sectionTitle\", \"\").lower()\n",
    "        caption = table.get(\"tableCaption\", \"\").lower()\n",
    "        headers = table.get(\"processed_tableHeaders\", [])\n",
    "        rows = table.get(\"tableData\", {})\n",
    "        entity_columns = table.get(\"entityColumn\", [])\n",
    "        headers = [headers[j] for j in entity_columns]\n",
    "        entity_cells = np.array(table.get(\"entityCell\", [[]]))\n",
    "        subject = table[\"subject_column\"]\n",
    "        core_entities = []\n",
    "        num_rows = len(rows)\n",
    "        for i in range(num_rows):\n",
    "            if entity_cells[i, subject] == 1:\n",
    "                entity = rows[i][subject][\"surfaceLinks\"][0][\"target\"][\"id\"]\n",
    "                entity_text = rows[i][subject][\"text\"]\n",
    "                core_entities.append([entity_text, entity])\n",
    "        core_entities = [z for z in core_entities if z[1] in all_entity_set]\n",
    "        if len(core_entities) < 5:\n",
    "            tables_ignored += 1\n",
    "            continue\n",
    "        seed_entities = [z[1] for z in core_entities[:seed_num]]\n",
    "        seed_entities_text = [z[0] for z in core_entities[:seed_num]]\n",
    "        target_entities = set([z[1] for z in core_entities[seed_num:]])\n",
    "        seeds_1, _, _, pall, pee, pce, ple, cand_e, cand_c = cached_baseline_result[table_id]\n",
    "        if len(target_entities) == 0:\n",
    "            tables_ignored += 1\n",
    "            continue\n",
    "        results[table_id] = {}\n",
    "        assert seeds_1 == set(seed_entities)\n",
    "        cand_e = set([z for z in cand_e if z in all_entity_set and z not in seed_entities])\n",
    "        cand_c = set([z for z in cand_c if z in all_entity_set and z not in seed_entities])\n",
    "        entity_cand = list(cand_e | cand_c)\n",
    "\n",
    "        pee = {k: v for k, v in pee.items() if k in entity_cand}\n",
    "        pce = {k: v for k, v in pce.items() if k in entity_cand}\n",
    "        ple = {k: v for k, v in ple.items() if k in entity_cand}\n",
    "        pall = {k: v for k, v in pall.items() if k in entity_cand}\n",
    "\n",
    "        (\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_mask,\n",
    "            input_ent,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent_type,\n",
    "            candidate_entity_set,\n",
    "        ) = CER_build_input(\n",
    "            pgEnt, pgTitle, secTitle, caption, headers[0], seed_entities, seed_entities_text, entity_cand, dataset\n",
    "        )\n",
    "\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        candidate_entity_set = candidate_entity_set.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ent_outputs = model(\n",
    "                input_tok,\n",
    "                input_tok_type,\n",
    "                input_tok_pos,\n",
    "                input_mask,\n",
    "                input_ent,\n",
    "                input_ent_text,\n",
    "                input_ent_text_length,\n",
    "                input_ent_type,\n",
    "                input_mask,\n",
    "                candidate_entity_set,\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "            ent_prediction_scores = ent_outputs[0][0].tolist()\n",
    "\n",
    "            p_neural = {}\n",
    "\n",
    "            for i, entity in enumerate(entity_cand):\n",
    "                p_neural[entity] = ent_prediction_scores[i]\n",
    "        results[table_id] = {\n",
    "            \"pgTitle\": pgTitle,\n",
    "            \"secTitle\": secTitle,\n",
    "            \"caption\": caption,\n",
    "            \"headers\": headers,\n",
    "            \"cand_all\": entity_cand,\n",
    "            \"cand_e\": cand_e,\n",
    "            \"cand_c\": cand_c,\n",
    "            \"seed_e\": seed_entities,\n",
    "            \"target_e\": target_entities,\n",
    "            \"p_neural\": p_neural,\n",
    "            \"pee\": pee,\n",
    "            \"pce\": pce,\n",
    "            \"ple\": ple,\n",
    "            \"pall\": pall,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(z[\"target_e\"]) for _, z in results.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"map neural\", np.mean([get_ap(x[\"p_neural\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\n",
    "    \"map neural - only cand_e\",\n",
    "    np.mean(\n",
    "        [\n",
    "            get_ap({z: score if z in x[\"cand_e\"] else -10000 for z, score in x[\"p_neural\"].items()}, x[\"target_e\"])\n",
    "            for _, x in results.items()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\"map ee\", np.mean([get_ap(x[\"pee\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\"map le\", np.mean([get_ap(x[\"ple\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\"map ce\", np.mean([get_ap(x[\"pce\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\"map all\", np.mean([get_ap(x[\"pall\"], x[\"target_e\"]) for _, x in results.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./output/CER/v2/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam_seed_0_10000/pytorch_model.bin\"\n",
    "\n",
    "\n",
    "config_class, model_class, _ = MODEL_CLASSES[\"CER\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True\n",
    "\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entity_set = set(dataset.entity_wikid2id.keys())\n",
    "tables_ignored = 0\n",
    "dev_result = {}\n",
    "cached_baseline = \"data/wikitables_v2/CER_test_result_seed_0.pkl\"\n",
    "with open(cached_baseline, \"rb\") as f:\n",
    "    cached_baseline_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 0\n",
    "results = {}\n",
    "with open(os.path.join(data_dir, \"test_tables.jsonl\"), \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        table = json.loads(line.strip())\n",
    "        table_id = table.get(\"_id\", \"\")\n",
    "        pgEnt = table[\"pgId\"]\n",
    "        if not pgEnt in all_entity_set:\n",
    "            pgEnt = -1\n",
    "        pgTitle = table.get(\"pgTitle\", \"\").lower()\n",
    "        secTitle = table.get(\"sectionTitle\", \"\").lower()\n",
    "        caption = table.get(\"tableCaption\", \"\").lower()\n",
    "        headers = table.get(\"processed_tableHeaders\", [])\n",
    "        rows = table.get(\"tableData\", {})\n",
    "        entity_columns = table.get(\"entityColumn\", [])\n",
    "        headers = [headers[j] for j in entity_columns]\n",
    "        entity_cells = np.array(table.get(\"entityCell\", [[]]))\n",
    "        subject = table[\"subject_column\"]\n",
    "        core_entities = []\n",
    "        num_rows = len(rows)\n",
    "        for i in range(num_rows):\n",
    "            if entity_cells[i, subject] == 1:\n",
    "                entity = rows[i][subject][\"surfaceLinks\"][0][\"target\"][\"id\"]\n",
    "                entity_text = rows[i][subject][\"text\"]\n",
    "                core_entities.append([entity_text, entity])\n",
    "        core_entities = [z for z in core_entities if z[1] in all_entity_set]\n",
    "        if len(core_entities) < 5:\n",
    "            tables_ignored += 1\n",
    "            continue\n",
    "        seed_entities = []\n",
    "        seed_entities_text = []\n",
    "        target_entities = set([z[1] for z in core_entities])\n",
    "        _, target, _, pall, _, pce, ple, _, cand_c = cached_baseline_result[table_id]\n",
    "        assert target == target_entities\n",
    "        if len(target_entities) == 0:\n",
    "            tables_ignored += 1\n",
    "            continue\n",
    "        results[table_id] = {}\n",
    "        cand_c = set([z for z in cand_c if z in all_entity_set])\n",
    "        entity_cand = list(cand_c)\n",
    "\n",
    "        pce = {k: v for k, v in pce.items() if k in entity_cand}\n",
    "        ple = {k: v for k, v in ple.items() if k in entity_cand}\n",
    "        pall = {k: v for k, v in pall.items() if k in entity_cand}\n",
    "\n",
    "        (\n",
    "            input_tok,\n",
    "            input_tok_type,\n",
    "            input_tok_pos,\n",
    "            input_mask,\n",
    "            input_ent,\n",
    "            input_ent_text,\n",
    "            input_ent_text_length,\n",
    "            input_ent_type,\n",
    "            candidate_entity_set,\n",
    "        ) = CER_build_input(\n",
    "            pgEnt, pgTitle, secTitle, caption, headers[0], seed_entities, seed_entities_text, entity_cand, dataset\n",
    "        )\n",
    "\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        candidate_entity_set = candidate_entity_set.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ent_outputs = model(\n",
    "                input_tok,\n",
    "                input_tok_type,\n",
    "                input_tok_pos,\n",
    "                input_mask,\n",
    "                input_ent,\n",
    "                input_ent_text,\n",
    "                input_ent_text_length,\n",
    "                input_ent_type,\n",
    "                input_mask,\n",
    "                candidate_entity_set,\n",
    "                None,\n",
    "                None,\n",
    "            )\n",
    "            ent_prediction_scores = ent_outputs[0][0].tolist()\n",
    "\n",
    "            p_neural = {}\n",
    "\n",
    "            for i, entity in enumerate(entity_cand):\n",
    "                p_neural[entity] = ent_prediction_scores[i]\n",
    "        results[table_id] = {\n",
    "            \"pgTitle\": pgTitle,\n",
    "            \"secTitle\": secTitle,\n",
    "            \"caption\": caption,\n",
    "            \"headers\": headers,\n",
    "            \"cand_c\": cand_c,\n",
    "            \"seed_e\": seed_entities,\n",
    "            \"target_e\": target_entities,\n",
    "            \"p_neural\": p_neural,\n",
    "            \"pce\": pce,\n",
    "            \"ple\": ple,\n",
    "            \"pall\": pall,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cached_baseline_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(z[\"target_e\"]) for _, z in results.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"recall all\",\n",
    "    np.mean([len(set(x[\"cand_all\"]) & x[\"target_e\"]) / len(x[\"target_e\"]) for _, x in results.items()]),\n",
    "    np.mean([len(set(x[\"cand_all\"])) for _, x in results.items()]),\n",
    ")\n",
    "print(\n",
    "    \"recall e\",\n",
    "    np.mean([len(x[\"cand_e\"] & x[\"target_e\"]) / len(x[\"target_e\"]) for _, x in results.items()]),\n",
    "    np.mean([len(set(x[\"cand_e\"])) for _, x in results.items()]),\n",
    ")\n",
    "print(\n",
    "    \"recall c\",\n",
    "    np.mean([len(x[\"cand_c\"] & x[\"target_e\"]) / len(x[\"target_e\"]) for _, x in results.items()]),\n",
    "    np.mean([len(set(x[\"cand_c\"])) for _, x in results.items()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap(scores, target_e):\n",
    "    ranked = sorted(scores.items(), key=lambda z: z[1], reverse=True)\n",
    "    ranked_l = [1 if z[0] in target_e else 0 for z in ranked]\n",
    "    ap = average_precision(ranked_l)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"map neural\", np.mean([get_ap(x[\"p_neural\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "# print('map neural - only cand_e', np.mean([get_ap({z:score if z in x['cand_e'] else -10000 for z, score in x['p_neural'].items()},x['target_e']) for _,x in results.items()]))\n",
    "# print('map ee', np.mean([get_ap(x['pee'],x['target_e']) for _,x in results.items()]))\n",
    "print(\"map le\", np.mean([get_ap(x[\"ple\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\"map ce\", np.mean([get_ap(x[\"pce\"], x[\"target_e\"]) for _, x in results.items()]))\n",
    "print(\"map all\", np.mean([get_ap(x[\"pall\"], x[\"target_e\"]) for _, x in results.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in [0.999, 0.99, 0.9, 0.5, 0.1, 0.05, 0.06, 0.07, 0.08, 0.09, 0.01]:\n",
    "    print(\n",
    "        \"map neural - ensemble {}\".format(w),\n",
    "        np.mean(\n",
    "            [\n",
    "                get_ap({z: w * score + (1 - w) * x[\"pee\"][z] for z, score in x[\"p_neural\"].items()}, x[\"target_e\"])\n",
    "                for _, x in results.items()\n",
    "            ]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ids = []\n",
    "for table_id, x in results.items():\n",
    "    recall = len(set(x[\"cand_all\"]) & x[\"target_e\"]) / len(x[\"target_e\"])\n",
    "    ap_neural = get_ap(x[\"p_neural\"], x[\"target_e\"])\n",
    "    ap_ee = get_ap(x[\"pee\"], x[\"target_e\"])\n",
    "    if recall != 0 and (ap_neural < 0.4 or ap_neural < ap_ee):\n",
    "        inspect_ids.append(table_id)\n",
    "print(len(inspect_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_result(result):\n",
    "    ap_neural = get_ap(result[\"p_neural\"], result[\"target_e\"])\n",
    "    ap_ee = get_ap(result[\"pee\"], result[\"target_e\"])\n",
    "    print(\"ap_neural: {}\\nap_ee: {}\".format(ap_neural, ap_ee))\n",
    "    print(\"{} - {} - {}\".format(result[\"pgTitle\"], result[\"secTitle\"], result[\"caption\"]))\n",
    "    print(result[\"headers\"])\n",
    "    print(\"seed:\")\n",
    "    print(\"; \".join([entity_vocab[entity_wikid2id[e]][\"wiki_title\"] for e in result[\"seed_e\"]]))\n",
    "    target_entities = [entity_vocab[entity_wikid2id[z]] for z in result[\"target_e\"]]\n",
    "    print(\"target:\\n%s\" % (\"; \".join([z[\"wiki_title\"] for z in target_entities])))\n",
    "    ranked_neural = sorted(result[\"p_neural\"].items(), key=lambda z: z[1], reverse=True)\n",
    "    print(\"neural:\")\n",
    "    print(\n",
    "        \"; \".join(\n",
    "            [\n",
    "                (\n",
    "                    \"[%s:%f]\" % (entity_vocab[entity_wikid2id[e]][\"wiki_title\"], score)\n",
    "                    if e in result[\"target_e\"]\n",
    "                    else \"%s:%.2f\" % (entity_vocab[entity_wikid2id[e]][\"wiki_title\"], score)\n",
    "                )\n",
    "                for e, score in ranked_neural[:10]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    ranked_e = sorted(result[\"pee\"].items(), key=lambda z: z[1], reverse=True)\n",
    "    print(\"ee:\")\n",
    "    print(\n",
    "        \"; \".join(\n",
    "            [\n",
    "                (\n",
    "                    \"[%s:%f]\" % (entity_vocab[entity_wikid2id[e]][\"wiki_title\"], score)\n",
    "                    if e in result[\"target_e\"]\n",
    "                    else \"%s:%.2f\" % (entity_vocab[entity_wikid2id[e]][\"wiki_title\"], score)\n",
    "                )\n",
    "                for e, score in ranked_e[:10]\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_result(results[inspect_ids[3]])\n",
    "print(len([id for id in inspect_ids if results[id][\"headers\"][0] in [\"opponent\", \"team 1\", \"home team\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_result(results[inspect_ids[6]])\n",
    "print(len([id for id in inspect_ids if \"miss dominican republic\" in results[id][\"pgTitle\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_result(results[inspect_ids[32]])\n",
    "print(len([id for id in inspect_ids if results[id][\"headers\"][0] == \"constituency\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, _ = MODEL_CLASSES[\"HR\"]\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WikiHeaderDataset(\n",
    "    data_dir, max_input_tok=350, src=\"train\", max_length=[50, 10], force_new=False, tokenizer=None\n",
    ")\n",
    "eval_dataset = WikiHeaderDataset(\n",
    "    data_dir, max_input_tok=350, src=\"dev\", max_length=[50, 10], force_new=False, tokenizer=None\n",
    ")\n",
    "test_dataset = WikiHeaderDataset(\n",
    "    data_dir, max_input_tok=350, src=\"test\", max_length=[50, 10], force_new=False, tokenizer=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset.header_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.__dict__[\"header_vocab_size\"] = len(eval_dataset.header_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"output/HR/v2/1/model_v1_table_0.2_0.6_0.7_10000_1e-4_candnew_0_adam\"\n",
    "# checkpoint = \"output/HR/bert_seed_0/\"\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(os.path.join(checkpoint, \"pytorch_model.bin\"))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 64\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = WikiHeaderLoader(\n",
    "    eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size, is_train=False, seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    _, input_tok, input_tok_type, input_tok_pos, input_mask, seed_header, target_header = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    seed_header = seed_header.to(device)\n",
    "    target_header = target_header.to(device)\n",
    "    # pdb.set_trace()\n",
    "    with torch.no_grad():\n",
    "        header_outputs = model(input_tok, input_tok_type, input_tok_pos, input_mask, seed_header, target_header)\n",
    "        header_loss = header_outputs[0]\n",
    "        header_prediction_scores = header_outputs[1]\n",
    "        results.extend(header_prediction_scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap(scores, target_e):\n",
    "    ranked = np.argsort(scores)[::-1]\n",
    "    target_e = set(target_e)\n",
    "    ranked_l = [1 if z in target_e else 0 for z in ranked]\n",
    "    ap = average_precision(ranked_l)\n",
    "    #     if ap<0.7:\n",
    "    #         display([train_dataset.header_vocab[z] for z in ranked[:10]])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = []\n",
    "for i, x in tqdm(enumerate(results)):\n",
    "    ap = get_ap(x, eval_dataset[i][6][1:])\n",
    "    #     if ap<0.7:\n",
    "    #         display([train_dataset.header_vocab[z] for z in eval_dataset[i][6]])\n",
    "    maps.append(ap)\n",
    "print(np.mean(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [i for i, ap in enumerate(maps) if ap < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([eval_dataset.header_vocab[x] for x in np.argsort(results[1])[::-1][:10]])\n",
    "display([eval_dataset.header_vocab[x] for x in eval_dataset[1][5][1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.tokenizer.decode(eval_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(i):\n",
    "    print(eval_dataset.tokenizer.decode(eval_dataset[i][1]))\n",
    "    print(maps[i])\n",
    "    print(\"; \".join([eval_dataset.header_vocab[x] for x in np.argsort(results[i])[::-1][:10]]))\n",
    "    print(\"; \".join([eval_dataset.header_vocab[x] for x in eval_dataset[i][5][1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(errors[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_loc = \"output/HR/hybrid/model_v1_table_0.2_0.4_0.7_30000_1e-4_with_cand_0_seed_0/\"\n",
    "with open(os.path.join(dump_loc, \"header_embedding.tsv\"), \"w\") as f_e, open(\n",
    "    os.path.join(dump_loc, \"header_names.tsv\"), \"w\", encoding=\"utf8\"\n",
    ") as f_n:\n",
    "    for i, name in eval_dataset.header_vocab.items():\n",
    "        f_n.write(\"{}\\n\".format(name))\n",
    "        f_e.write(\"{}\\n\".format(\"\\t\".join([str(z) for z in model.cls.weight.data[i].tolist()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x, token_pattern=None)\n",
    "train_tfidf = tfidf.fit_transform([x[1][1 : x[5] + 1] for x in train_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=1, metric=\"cosine\", n_jobs=None)\n",
    "neigh.fit(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
