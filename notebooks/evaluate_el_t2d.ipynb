{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2D Evaluation\n",
    "\n",
    "This notebook evaluate the fine-tuned Entity-Linking (Cell Entity Linking, CEA) model, finetuned running the `./tasks/cea/fine_tune_EL.sh` script and evaluated with the `./notebooks/evaluate_task.ipynb` notebook.\n",
    "\n",
    "The data needed for this notebook are:\n",
    "\n",
    "* T2D `tables_instance_context` folder, which can be downloaded from https://webdatacommons.org/webtables/tables_instance_context.tar.gz\n",
    "* T2D `entities_instance` folder, which can be downloaded from https://webdatacommons.org/webtables/entities_instance.tar.gz\n",
    "* The english DBedia instances types, which can be downloaded from https://databus.dbpedia.org/dbpedia/mappings/instance-types/2019.08.30/instance-types_lang=en.ttl.bz2 \n",
    "* The results pickle obtained from the evaluation notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from operator import add\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 14:49:41 WARN Utils: Your hostname, chronos-gpu1 resolves to a loopback address: 127.0.1.1; using 10.0.0.113 instead (on interface ens18)\n",
      "24/02/22 14:49:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 14:49:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [\n",
    "        (\"spark.executor.memory\", \"8g\"),\n",
    "        (\"spark.executor.cores\", \"2\"),\n",
    "        (\"spark.executor.instances\", \"7\"),\n",
    "        (\"spark.driver.memory\", \"150g\"),\n",
    "        (\"spark.driver.maxResultSize\", \"100g\"),\n",
    "        (\"spark.driver.extraClassPath\", \"~/Downloads/sqlite-jdbc-3.36.0.3.jar\"),\n",
    "    ]\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikidata_lookup(query, retry: int = 3):\n",
    "    service_url = (\n",
    "        \"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={}&language=en&limit=50&format=json\"\n",
    "    )\n",
    "    url = service_url.format(urllib.parse.quote(query))\n",
    "    for _ in range(retry):\n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.code == 429 or e.code == 503:\n",
    "                response = e.code\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            else:\n",
    "                response = e.code\n",
    "                break\n",
    "        except urllib.error.URLError as e:\n",
    "            response = None\n",
    "            break\n",
    "        else:\n",
    "            response = json.loads(response.read())\n",
    "            break\n",
    "    if isinstance(response, dict):\n",
    "        response = [z.get(\"id\") for z in response.get(\"search\", [])]\n",
    "    return [query, response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|wikipedia_id|     wikipedia_title|wikidata_id|\n",
      "+------------+--------------------+-----------+\n",
      "|          10| AccessibleComputing|   Q3097841|\n",
      "|          12|           Anarchism|      Q6199|\n",
      "|          13|  AfghanistanHistory|    Q188872|\n",
      "|          14|AfghanistanGeography|   Q1637198|\n",
      "|          15|   AfghanistanPeople|   Q1075999|\n",
      "|          18|AfghanistanCommun...|   Q2658920|\n",
      "|          19|AfghanistanTransp...|    Q509443|\n",
      "|          20| AfghanistanMilitary|  Q11062919|\n",
      "|          21|AfghanistanTransn...|   Q4113710|\n",
      "|          23| AssistiveTechnology|    Q688498|\n",
      "|          24|        AmoeboidTaxa|    Q506524|\n",
      "|          25|              Autism|     Q38404|\n",
      "|          27|      AlbaniaHistory|    Q213833|\n",
      "|          29|       AlbaniaPeople|    Q583150|\n",
      "|          30|        AsWeMayThink|    Q610709|\n",
      "|          35|   AlbaniaGovernment|    Q917351|\n",
      "|          36|      AlbaniaEconomy|      Q8055|\n",
      "|          39|              Albedo|    Q101038|\n",
      "|          40|AfroAsiaticLanguages|     Q25268|\n",
      "|          42|  ArtificalLanguages|     Q33215|\n",
      "+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can create the index-enwiki dump use this library https://github.com/jcklie/wikimapper\n",
    "wikipedia_wikidata_mapping = (\n",
    "    spark.read.format(\"jdbc\")\n",
    "    .options(\n",
    "        url=\"jdbc:sqlite:~/turl-data/index_enwiki-20190420.db\",\n",
    "        driver=\"org.sqlite.JDBC\",\n",
    "        dbtable=\"mapping\",\n",
    "    )\n",
    "    .load()\n",
    ")\n",
    "wikipedia_wikidata_mapping.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_types = dict(\n",
    "    spark.createDataFrame(\n",
    "        sc.textFile(\"~/turl-data/dbpedia_types/2022_12_01/instance_type_en.ttl\")\n",
    "        .map(lambda x: x.split())\n",
    "        .map(\n",
    "            lambda x: Row(\n",
    "                wikipedia_title=unquote(x[0][1:-1]).replace(\"http://dbpedia.org/resource/\", \"\"),\n",
    "                type=x[2][1:-1].split(\"/\")[-1],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")\n",
    "    .rdd.map(lambda x: (x[\"wikidata_id\"], [x[\"type\"]]))\n",
    "    .reduceByKey(add)\n",
    "    .collect()\n",
    ")\n",
    "print(len(dbpedia_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables = sc.wholeTextFiles(\"~/turl-data/efthymiou/t2d/tables_instance_context\").map(\n",
    "    lambda x: (x[0].split(\"/\")[-1][:-5], json.loads(x[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities = spark.createDataFrame(\n",
    "    sc.wholeTextFiles(\"~/turl-data/efthymiou/t2d/entities_instance\")\n",
    "    .map(lambda x: (x[0].split(\"/\")[-1][:-4], list(csv.reader(x[1].split(\"\\n\")))))\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            Row(\n",
    "                table_id=x[0],\n",
    "                wikipedia_title=y[0].split(\"/\")[-1],\n",
    "                j=0,\n",
    "                i=int(y[2]),\n",
    "                mention=y[1].replace(\"&nbsp;\", \"\").replace(\"&nbsp\", \"\"),\n",
    "            )\n",
    "            for y in x[1]\n",
    "            if len(y) == 3\n",
    "        ]\n",
    "    )\n",
    ").join(wikipedia_wikidata_mapping, \"wikipedia_title\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entity_mentions = list(set(t2d_entities.rdd.map(lambda x: x[\"mention\"]).collect()))\n",
    "print(len(t2d_entity_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entity_mentions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_lookup(\"Barack Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "\n",
    "num_processes = 16\n",
    "entity_t2d_candidates = process_map(wikidata_lookup, t2d_entity_mentions, max_workers=num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_t2d_candidates[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity candidates from `wikidata_lookup`\n",
    "\n",
    "entity_t2d_candidates_rows = []\n",
    "for x in entity_t2d_candidates:\n",
    "    entity_t2d_candidates_rows.append(Row(mention=x[0], candidates=x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_t2d_candidates_df = spark.createDataFrame(entity_t2d_candidates_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities_with_candidates = t2d_entities.join(entity_t2d_candidates_df, \"mention\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities_with_candidates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset for testing with TURL El model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    t2d_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(t2d_tables)\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_for_own(x):\n",
    "    all_processed = []\n",
    "    table_id = x[0]\n",
    "    pgTitle = x[1][1][\"pageTitle\"]\n",
    "    secTitle = \"\"\n",
    "    caption = x[1][1][\"title\"]\n",
    "    header_i = x[1][1][\"headerRowIndex\"]\n",
    "    subject_j = x[1][1][\"keyColumnIndex\"]\n",
    "    headers = [column[header_i] for column in x[1][1][\"relation\"][subject_j:]]\n",
    "    all_entities = x[1][0]\n",
    "    total_num = len(all_entities)\n",
    "    chunck_num = int(total_num / max([1, int(total_num / 25)])) + 1\n",
    "    while len(all_entities) > 0:\n",
    "        entities = []\n",
    "        candidate_entities = {}\n",
    "        labels = []\n",
    "        cand_for_each = []\n",
    "        for e in all_entities[:chunck_num]:\n",
    "            row_i = e[0]\n",
    "            e_mention = e[2]\n",
    "            entities.append([[row_i, 0], e_mention])\n",
    "            for cand in e[4]:\n",
    "                if cand[0] not in candidate_entities:\n",
    "                    candidate_entities[cand[0]] = [\n",
    "                        len(candidate_entities),\n",
    "                        cand[1],\n",
    "                        cand[2],\n",
    "                        dbpedia_types.get(cand[0], []),\n",
    "                    ]\n",
    "            labels.append(candidate_entities[e[3]][0])\n",
    "            cand_for_each.append([candidate_entities[cand[0]][0] for cand in e[4]])\n",
    "            for p, column in enumerate(x[1][1][\"relation\"][subject_j + 1 : subject_j + 3]):\n",
    "                if len(column) > row_i:\n",
    "                    e_mention = column[row_i].replace(\"&nbsp;\", \"\").replace(\"&nbsp\", \"\")\n",
    "                    entities.append([[row_i, p + 1], e_mention])\n",
    "                    labels.append(0)\n",
    "                    cand_for_each.append([])\n",
    "                # entities = [[[z[0],0],z[2]] for z in all_entities[:50]]\n",
    "                # candidate_entities = {}\n",
    "                # for z in all_entities[:50]:\n",
    "                #     for cand in z[4]:\n",
    "                #         if cand[0] not in candidate_entities:\n",
    "                #             candidate_entities[cand[0]] = [len(candidate_entities),cand[1],cand[2],dbpedia_types.get(cand[0],[])]\n",
    "                # labels = [candidate_entities[z[3]][0]  for z in all_entities[:50]]\n",
    "                # cand_for_each = [[candidate_entities[cand[0]][0] for cand in z[4]] for z in all_entities[:50]]\n",
    "        tmp_candidate_entities = [0] * len(candidate_entities)\n",
    "        for k, v in candidate_entities.items():\n",
    "            tmp_candidate_entities[v[0]] = v[1:]\n",
    "        all_processed.append(\n",
    "            [table_id, pgTitle, secTitle, caption, headers, entities, tmp_candidate_entities, labels, cand_for_each]\n",
    "        )\n",
    "        all_entities = all_entities[chunck_num:]\n",
    "    return all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_local = (\n",
    "    t2d_entities_with_candidates.select(\"table_id\", \"wikidata_id\", \"candidates\", \"i\", \"j\", \"mention\")\n",
    "    .where(~F.isnull(\"candidates\"))\n",
    "    .rdd.map(lambda x: [x[\"table_id\"], x[\"i\"], x[\"j\"], x[\"mention\"], x[\"wikidata_id\"], x[\"candidates\"]])\n",
    "    .filter(lambda x: x[4] in [z for z in x[5]])\n",
    "    .map(lambda x: (x[0], [x[1:]]))\n",
    "    .reduceByKey(add)\n",
    "    .join(t2d_tables)\n",
    "    .flatMap(build_for_own)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/srv/samba/group_workspace_1/deng.595/workspace/table_transformer/data/wikitable_entity/v2/\"\n",
    "with open(data_dir + \"t2d.table_entity_linking.json\", \"w\") as f:\n",
    "    json.dump(t2d_local, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open an already created T2D dataset (this comes from the authors data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_candidate(tables):\n",
    "    results = []\n",
    "    # For every entity mention in the table\n",
    "    for i, entity in enumerate(tables[5]):\n",
    "        # If the candidate entities for the mention are empty, skip\n",
    "        if len(tables[8][i]) == 0:\n",
    "            continue\n",
    "        # ((table_id, entity row, entity col), [entity label, candidate indexes, candidate entities])\n",
    "        results.append(((tables[0], entity[0][0], entity[0][1]), [tables[7][i], tables[8][i], tables[6]]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"~/turl-data/round1_t2d.table_entity_linking.json\", \"rb\") as f:\n",
    "    t2d_local = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001: A Space Odyssey – Music from the Motion Picture Sound Track',\n",
       " '1968 compilation soundtrack album; various artists',\n",
       " ['Album']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2d_local[0][6][19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a single table contained in `t2d_local`\n",
    "\n",
    "* **Table ID**: '23235546-1'\n",
    "* **Page title**: 'Ivan Lendl career statistics'\n",
    "* **Section title**: 'Singles: 19 finals (8 titles, 11 runner-ups)'\n",
    "* **Caption**: ''\n",
    "* **Headers**: ['outcome', 'year', ...]\n",
    "* **Entity mentions**: [[[0, 4], 'Björn Borg'], [[9, 2], 'Wimbledon'], ...], with [[`row`, `col`], entity mention text]. `row` and `col` both starts from 0\n",
    "* **Candidate entities**: [['Björn Borg', 'Swedish tennis player', []], ['Björn Borg', 'Swedish swimmer', ['Swimmer']], ...], this the merged set for all cells. [entity name, entity description, entity types]\n",
    "* **Labels**: [0, 12, ...], this is the index of the gold entity in the candidate entities\n",
    "* **Candidate indexes**: [[0, 1, ...], [11, 12, 13, ...], ...], candidates for each mention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    label = 1 if x[\"wikidata_id\"] in [z for z in x[\"candidates\"][:1]] else 0\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_entities_with_atleast_one_candidate = t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_all_predicted = t2d_entities_with_atleast_one_candidate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_TP = t2d_entities_with_atleast_one_candidate.rdd.map(lambda x: classify(x)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_P = t2d_entities_with_candidates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_best_TP = t2d_entities_with_atleast_one_candidate.rdd.map(\n",
    "    lambda x: 1 if x[\"wikidata_id\"] in [z for z in x[\"candidates\"]] else 0\n",
    ").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_best_TP, t2d_all_predicted, t2d_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = t2d_TP / t2d_all_predicted\n",
    "recall = t2d_TP / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = t2d_best_TP / t2d_all_predicted\n",
    "recall = t2d_best_TP / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"~/projects/TURL/output/logs/turl/fine-tuning-el/2024-02-14_11-01-08/version_0/test/round1_t2d_entity_linking_results_dedup.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    test_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(result):\n",
    "    result = result[1]\n",
    "    # result[0] contains: label index (in the candidate list), candidate span (in the candidate list), candidates\n",
    "    # result[1] contains: sorted predicted indexes, sorted predicted scores\n",
    "    pred = []\n",
    "    lookup = [result[0][1][0], 0]  # Lookup the first candidate\n",
    "    # The prediction is first predicted candidate\n",
    "    # TODO: consider the case where the first predicted candidate is not in the candidate span, i.e.\n",
    "    # a totally different entity has been predicted\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x in result[0][1]:\n",
    "            pred = [x, result[1][1][i]]\n",
    "            break\n",
    "    # Get the score of the correct candidate\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x == lookup[0]:\n",
    "            lookup[1] = result[1][1][i]\n",
    "            break\n",
    "    final = pred[0] if pred[0] == lookup[0] or (pred[1] * 0.8) > lookup[1] else lookup[0]\n",
    "    if final == result[0][0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_sample = sc.parallelize(test_results).flatMap(\n",
    "    lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/20 09:47:12 WARN TaskSetManager: Stage 3 contains a task of very large size (6045 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample_result = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_result = sample_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('50245608_0_871275842592178099', 146, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_result[0]  # (table_id, entity row, entity col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sample_result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.76685333251953"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup: [1867, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"lookup:\", [result[0][1][0], 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tp(sample_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/20 09:47:54 WARN TaskSetManager: Stage 5 contains a task of very large size (6045 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "our_tp = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .map(get_tp)\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6261"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = our_tp / t2d_all_predicted\n",
    "recall = our_tp / t2d_P\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp(result):\n",
    "    result = result[1]\n",
    "    pred = []\n",
    "    lookup = [result[0][1][0], 0]\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x in result[0][1]:\n",
    "            pred = [x, result[1][1][i]]\n",
    "            break\n",
    "    for i, x in enumerate(result[1][0]):\n",
    "        if x == lookup[0]:\n",
    "            lookup[1] = result[1][1][i]\n",
    "            break\n",
    "    final = pred[0] if pred[0] == lookup[0] or (pred[1] * 0.8) > lookup[1] else lookup[0]\n",
    "    if final == result[0][0]:\n",
    "        return (1, result[0][2][final])\n",
    "    else:\n",
    "        return (0, result[0][2][final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .take(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0][1][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_results = (\n",
    "    sc.parallelize(t2d_local)\n",
    "    .flatMap(get_labels_and_candidate)\n",
    "    .join(\n",
    "        sc.parallelize(test_results).flatMap(\n",
    "            lambda x: [((x[0], z[0], z[1]), (x[2][i], x[3][i])) for i, z in enumerate(x[1])]\n",
    "        )\n",
    "    )\n",
    "    .map(lambda x: (x[0], get_tp(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_results = t2d_entities_with_candidates.where(F.size(\"candidates\") >= 1).rdd.map(\n",
    "    lambda x: (\n",
    "        (x[\"table_id\"], x[\"i\"], x[\"j\"]),\n",
    "        (x[\"mention\"], x[\"candidates\"], 1 if x[\"wikidata_id\"] in [z for z in x[\"candidates\"][:1]] else 0),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = our_results.join(lookup_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.take(1)[0][1][1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = all_results.filter(lambda x: x[1][0] == 0 and x[1][1][-1] == 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = all_results.filter(lambda x: x[1][0] == 1 and x[1][1][-1] == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(errors))\n",
    "print(len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([x[0][0] for x in errors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([x[0][0] for x in correct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([x[0][0] for x in errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[x, t2d_tables_local[x][\"pageTitle\"]] for x in list(set([x[0][0] for x in errors]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in errors if x[0][0] == \"41194422_0_7231546114369966811\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables_local = dict(t2d_tables.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2d_tables_local[\"71137051_0_8039724067857124984\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
